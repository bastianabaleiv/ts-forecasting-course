---
title: "Clase 4"
description: |
  Pronosticando Series de Tiempo en R con modelos univariados
author:
  - name: Basti谩n Aballay L.
    url: https://www.linkedin.com/in/bastianaballay/
date: "2021-11-15"
bibliography: clase-04.bib
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: true
    highlight: tango
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r packages, include=FALSE}
suppressPackageStartupMessages(library(tidyverse))

library(xaringanExtra)
library(timetk)
library(quantmod)

xaringanExtra::use_panelset()
```

En la [Clase 3](https://hapst.netlify.app/posts/clase-03) pronosticamos series de tiempo utilizando m茅todos *ingenuos* y regresiones lineales. Si bien el modelo de regresi贸n lineal no fue dise帽ado para manipular y pronosticar series de tiempo, es posible generar caracter铆sticas^[*feature engineering*] y transformar el problema de pron贸stico en un problema de regresi贸n. Sin embargo, el supuesto para ajustar un modelo de regresi贸n por m铆nimos cuadrados ordinarios (MCO)^[(OLS): *Ordinary Least Squares*] acerca del t茅rmino del error $\epsilon$ (independiente e id茅nticamente distribuido con media $0$ y varianza $\sigma^2$) en muchos casos es violado al trabajar con series de tiempo. Existe la posibilidad de que los residuales del modelo a煤n presenten correlaci贸n^[Debemos relajar el supuesto de independencia de las muestras obtenidas a partir de la poblaci贸n], por lo que necesitamos disponer de modelos que aborden dichas dependencias en el tiempo de manera natural.

En esta clase estudiaremos modelos univariados para pron贸stico de series de tiempo, revisando los principios te贸ricos y pr谩cticos de cada uno de ellos utilizando ecosistemas de `R`. Luego de esta clase podr谩s:

+ Utilizar modelos de suavizamiento exponencial (*Exponential Smoothing Models*) para pronosticar series de tiempo
+ Utilizar modelos autoregresivos (`AR`), de medias m贸viles (`MA`), integrados (`ARIMA`) y estacionales (`SARIMA`) para pronosticar series de tiempo.
+ Modelar residuales de una regresi贸n lineal con modelos de series de tiempo
+ Abordar estacionalidades complejas con modelos adecuados (`bats` y `tbats`)
+ Utilizar modelos de regresi贸n no-lineal como Prophet

# Suavizamiento Exponencial (Exponential Smoothing)^[Este segmento es un resumen del siguiente cap铆tulo de `fpp3`: [https://otexts.com/fpp3/expsmooth.html](https://otexts.com/fpp3/expsmooth.html)] 

El *Suavizamiento Exponencial* fue propuesto a finales de los 50's (Brown, Holt, Winters) y ha motivado el desarrollo de m茅todos de pron贸sticos exitosos en el dominio de series de tiempo. Los pron贸sticos producidos por m茅todos de suavizamiento exponencial son *medias ponderadas de observaciones pasadas*, con pesos^[ponderadores] que decaen de manera exponencial a medida que las observaciones se hacen m谩s distantes en el tiempo. Es decir, las observaciones recientes poseen mayor relevancia para efectos de elaboraci贸n de una predicci贸n para valores futuros.

<aside>
Exponential Smoothing
</aside>

## Suavizamiento Exponencial Simple (SES)

El *Suavizamiento Exponencial Simple* es un m茅todo adecuado para pronosticar datos que no poseen un patr贸n de tendencia o estacionalidad claro, siendo un m茅todo simple y r谩pido de computar. SES supone que la serie de tiempo s贸lo contiene un componente de un *nivel* $\ell_t$ (*level*), que se mantiene en el tiempo y un error. Podemos representar el suavizamiento exponencial simple utilizando ecuaciones de pron贸stico y suavizamiento de la siguiente manera^[https://otexts.com/fpp3/ses.html]:

$$
\begin{align*}
  \text{Ecuaci贸n de Pron贸stico}  && \hat{y}_{t+h|t} & = \ell_{t}\\
  \text{Ecuaci贸n de Suavizamiento} && \ell_{t}        & = \alpha y_{t} + (1 - \alpha)\ell_{t-1},
\end{align*}
$$

Podemos ver que el pron贸stico para SES corresponde al nivel estimado para la observaci贸n m谩s reciente en per铆odo $t$. Para obtener el nivel $\ell_t$, utilizamos la ecuaci贸n de suavizamiento, que actualiza el nivel anterior $\ell_{t-1}$ mediante la integraci贸n de la informaci贸n m谩s reciente asociada a la observaci贸n $y_t$. En este caso, $\alpha$ es el ponderador o la *constante de suavizamiento* que toma valores entre $0 \leq \alpha \leq 1$. El algoritmo *aprende* el nuevo nivel a partir de los datos observados m谩s recientes. Debemos notar que es necesario *inicializar* el algoritmo definiendo una condici贸n inicial para $\ell_1$^[Una opci贸n es definir el punto de partida como el primer valor observado, $\ell_1 = y_1$]. Si sustituimos $\ell_t$ en su f贸rmula obtenemos:

\begin{align*}
  \ell_{t}   & = \alpha y_t + (1-\alpha) \left[\alpha y_{t-1} + (1-\alpha) \ell_{t-2}\right]    \\
                   & = \alpha y_t + \alpha(1-\alpha) y_{t-1} + (1-\alpha)^2 \ell_{t-2}                \\
                   & = \dots                                                                          \\
                   & = \alpha y_t + \alpha(1-\alpha) y_{t-1} + \alpha(1-\alpha)^2 y_{t-2} + \dots
\end{align*}

en este caso, terminamos con un promedio ponderado de todos los valores de la serie con pesos que decaen de manera exponencial^[De ah铆 el nombre del m茅todo].

La constante de suavizamiento $\alpha$ determina qu茅 tanto peso^[o importancia] se otorga a una observaci贸n pasada:

+ $\alpha = 1$: observaciones pasadas no influyen en los pron贸sticos^[No hay aprendizaje por parte del modelo]
+ $\alpha = 0$: observaciones pasadas poseen una influencia equitativa en los pron贸sticos 

En la literatura, el par谩metro $\alpha$ posee como valores t铆picos `0.1` o `0.2`^[Con valores de $\alpha$ altos, el decaimiento es lento.] . Sin embargo, es posible obtener su valor 贸ptimo minimizando alguna m茅trica de desempe帽o de inter茅s, con el cuidado de no sobre-ajustar el m茅todo a los datos.

Finalmente, es posible entender SES como un proceso de aprendizaje adaptativo si consideramos la ecuaci贸n del nivel de la siguiente forma:

$$
\ell_{t} = \alpha y_t + (1-\alpha)\ell_{t-1} = \alpha y_t + \ell_{t-1} - \alpha \ell_{t-1} 
$$

as铆, podemos sustituir $\ell_{t}$ por $\hat{y}_{t+h|t}$

\begin{align}
\hat{y}_{t+1|t} & = \ell_{t} = \ell_{t-1} + \alpha(y_{t} - \ell_{t-1}) \\
                & = \hat{y}_{t+1|t} + \alpha(y_t - \hat{y}_{t+1|t}) \\
                & = \hat{y}_{t+1|t} + \alpha \varepsilon_t
\end{align}

y notamos que el pron贸stico es una actualizaci贸n del pron贸stico anterior m谩s un componente que depende del error de dicho pron贸stico anterior.

```{r}
library(USgas)
library(dplyr)
library(parsnip)
library(rsample)
library(timetk)
library(modeltime)

# The us_monthly dataset provides a monthly time series, 
# representing the demand for natural gas in the US between 2001 and 2020:
data("us_monthly")
us_monthly %>%  rmarkdown::paged_table()

splits <- us_monthly %>% 
  timetk::tk_tbl() %>% 
  initial_time_split(prop = 0.8)
```

```{r}
# SES
model_spec_ses <- exp_smoothing( # SES
  error = "additive",
  trend = "none",
  seasonal = "none"
) %>%
  set_engine("ets")

model_fit_ses <- model_spec_ses %>%
  fit(
    y ~ date,
    data = training(splits)
  )

model_fit_ses
```

```{r}
calibration_tbl <- model_fit_ses %>%
  # Calibracion en datos de test para evaluacion de rendimiento
  modeltime_calibrate(new_data = testing(splits)) %>%
  # Pronostico en datos de test
  modeltime_forecast(
    new_data    = testing(splits),
    actual_data = us_monthly
  )
calibration_tbl %>% rmarkdown::paged_table()

calibration_tbl %>% 
  # Grafico pronosticos
  plot_modeltime_forecast(.interactive = TRUE)
```

## M茅todos con Tendencia 

### Tendencia Lineal de Holt

En 1957, Holt extendi贸 SES para permitir que los pron贸sticos pudiesen incorporar efectos de tendencia. Este m茅todo involucra una ecuaci贸n de pron贸stico, una ecuaci贸n de nivel y otra de tenencia seg煤n:

\begin{align*}
  \text{Ecuaci贸n de pron贸stico}&& \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} \\
  \text{Ecuaci贸n de nivel}   && \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  \text{Ecuaci贸n de tendencia}   && b_{t}    &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)b_{t-1},
\end{align*}

donde $b_t$ denota al estimador de la tendencia (pendiente) de la serie en el per铆odo $t$ cuyo par谩metro de suavizamiento es $\beta^*$, que toma valores entre $0 \leq \beta^* \leq 1$.

Tal como con SES, la ecuaci贸n de nivel muestra que $\ell_t$ es un promedio ponderado de la observaci贸n $y_t$ y el pron贸stico a un paso para el per铆odo $t$, dado por $\ell_t + b_{t-1}$. La ecuaci贸n de la tendencia muestra que $b_t$ es un promedio ponderado de la tendencia estimada para el per铆odo $t$ basada en $\ell_t - \ell_{t-1}$ y $b_{t-1}$, el estimador previo de la tendencia. Es decir, la tendencia previa se actualiza utilizando la diferencia entre los valores del nivel m谩s actuales. En ese sentido, podemos hablar de una *tendencia local* que se adapta a trav茅s del tiempo mediante el par谩metro de velocidad $\beta^*$.

En este modelo, el pron贸stico ya no es plano, sino posee tendencia, donde el pron贸stico a $h$ pasos es equivalente al 煤ltimo nivel estimado m谩s $h$ veces el 煤ltimo valor de la tendencia estimada^[Es decir, los pron贸sticos son una funci贸n lineal de $h$].  

```{r}
# HOLT
model_spec_holt <- exp_smoothing( # SES
  error = "additive",
  trend = "additive",
  seasonal = "none"
) %>%
  set_engine("ets")

model_fit_holt <- model_spec_holt %>%
  fit(
    y ~ date,
    data = training(splits)
  )

model_fit_holt
```

```{r}
calibration_tbl <- model_fit_holt %>%
  # Calibracion en datos de test para evaluacion de rendimiento
  modeltime_calibrate(new_data = testing(splits)) %>%
  # Pronostico en datos de test
  modeltime_forecast(
    new_data    = testing(splits),
    actual_data = us_monthly
  )
calibration_tbl %>% rmarkdown::paged_table()

calibration_tbl %>% 
  # Grafico pronosticos
  plot_modeltime_forecast(.interactive = TRUE)
```

### Tendencia amortiguada

Como pudimos ver, los pron贸sticos generados por el m茅todo de Holt muestran una tendencia constante (creciente o decreciente) hacia el futuro, lo que a largo plazo puede tender a sobreestimar valores futuros. La introducci贸n de un par谩metro $\phi$ para *amortiguar* (*damping*)^[Gardner & McKenzie (1985)] la tendencia a una l铆nea plana en el futuro:

\begin{align*}
  \hat{y}_{t+h|t} &= \ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t} \\
  \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)\phi b_{t-1}.
\end{align*}

donde $0 < \phi < 1$. Si $\phi = 1$ obtenemos el m茅todo lineal de Holt.

```{r}
# HOLT DAMPED
model_spec_holt_damped <- exp_smoothing( # SES
  error = "additive",
  trend = "additive",
  seasonal = "none",
  damping = "damped"
) %>%
  set_engine("ets")

model_fit_holt_damped  <- model_spec_holt_damped  %>%
  fit(
    y ~ date,
    data = training(splits)
  )

model_fit_holt_damped
```

```{r}
calibration_tbl <- model_fit_holt_damped %>%
  # Calibracion en datos de test para evaluacion de rendimiento
  modeltime_calibrate(new_data = testing(splits)) %>%
  # Pronostico en datos de test
  modeltime_forecast(
    new_data    = testing(splits),
    actual_data = us_monthly
  )
calibration_tbl %>% rmarkdown::paged_table()

calibration_tbl %>% 
  # Grafico pronosticos
  plot_modeltime_forecast(.interactive = TRUE)
```

Lamentablemente, para la serie de `USgas` el modelo no presenta mayor aporte, sin embargo, podemos ver un ejemplo de aplicaci贸n para pronosticar la poblaci贸n australiana  [ac谩](https://otexts.com/fpp3/holt.html#example-australian-population-continued).

## M茅todos con Estacionalidad

Consideremos los tipos de tendencia y estacionalidad del siguiente [video](https://youtu.be/DUyZl-abnNM?t=92). En el caso de *tendencia aditiva*, la tendencia crece/decrece de manera lineal a trav茅s del tiempo. La *tendencia multiplicativa* implica crecimiento/decrecimiento exponencial en el tiempo. Finalmente, la *tendencia polinomial* permite capturar una variedad de formas de manera flexible. De manera similar, hallamos *estacionalidad aditiva*, donde cada estaci贸n difere en una cantidad espec铆fica de otra y *estacionalidad multiplicativa*, donde cada estaci贸n difere de otra en un porcentaje espec铆fico.

Podemos entender un modelo aditivo como la suma de los componentes sistem谩ticos de una serie m谩s el componente de ruido o no-sistem谩tico de la siguiente forma:

$$y_t = \text{nivel}\ + \text{tendencia}\ + \text{estacionalidad}\ + \text{ruido}$$

y a los modelos multiplicativos seg煤n:

$$y_t = \text{nivel}\ \times \text{tendencia}\ \times \text{estacionalidad}\ \times \text{ruido}$$

En 1960 Holt y Winters extendieron el m茅todo de Holt para capturar efectos estacionales. El m茅todo estacional de Holt-Winters involucra una ecuaci贸n de pron贸stico y tres ecuaciones de suavizamiento para el nivel $\ell_t$, la tendencia $b_t$ y la estacionalidad $s_t$, con par谩metros $\alpha$, $\beta*$ y $\gamma$, respectivamente. En este caso, usamos $m$ para denotar el per铆odo asociado a la estacionalidad.

### Holt-Winters Aditivo

Cuando las variaciones estacionales son relativamente constantes a trav茅s de la serie es preferible utilizar el modelo de Holt-Winters Aditivo. En este caso, la componente estacional se expresa en t茅rminos absolutos seg煤n la escala de la serie observada, y la ecuaci贸n del nivel es ajustada estacionalmente mediante la sustracci贸n del componente estacional, como sigue:

\begin{align*}
  \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} + s_{t+h-m(k+1)} \\
  \ell_{t} &= \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)b_{t-1}\\
  s_{t} &= \gamma (y_{t}-\ell_{t-1}-b_{t-1}) + (1-\gamma)s_{t-m},
\end{align*}

donde $k$ es la parte entera de $(h-1)/m$^[Asegura que los estimadores de los indices estacionales usados para pronosticar provengan del 煤ltimo periodo estacional de la muestra.]. La ecuaci贸n del nivel presenta un promedio ponderado entre la observaci贸n ajustada por estacionalidad y el pron贸stico no-estacional ($\ell_{t-1}+b_{t-1}$) para el per铆odo $t$. La ecuaci贸n de la tendencia es la misma que para el m茅todo de Holt. LA ecuaci贸n de estacionalidad muestra un promedio ponderado entre el indice estacional actual, , y el 铆ndice estacional de la misma temporada en el 煤ltimo per铆odo^[Es decir, hace $m$ per铆odos atr谩s.]. 

```{r}
# HOLT WINTERS aditivo
model_spec_hw <- exp_smoothing(
  error = "additive",
  trend = "additive",
  season = "additive",
  seasonal_period = 12
) %>%
  set_engine("ets")

model_fit_hw  <- model_spec_hw  %>%
  fit(
    y ~ date,
    data = training(splits)
  )

model_fit_hw
```

```{r}
calibration_tbl <- model_fit_hw %>%
  # Calibracion en datos de test para evaluacion de rendimiento
  modeltime_calibrate(new_data = testing(splits)) %>%
  # Pronostico en datos de test
  modeltime_forecast(
    new_data    = testing(splits),
    actual_data = us_monthly
  )
calibration_tbl %>% rmarkdown::paged_table()

calibration_tbl %>% 
  # Grafico pronosticos
  plot_modeltime_forecast(.interactive = TRUE)
```

### Holt-Winters Multiplicativo

Por otra parte, el modelo multiplicativo es preferido cuando las variaciones estacionales cambian de manera proporcional con el nivel de la serie. En este caso, la componente estacional se expresa en t茅rminos relativos (porcentajes), y la serie es ajustada estacionalmente mediante la divisi贸n a trav茅s del componente estacional.

\begin{align*}
  \hat{y}_{t+h|t} &= (\ell_{t} + hb_{t})s_{t+h-m(k+1)} \\
  \ell_{t} &= \alpha \frac{y_{t}}{s_{t-m}} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t}-\ell_{t-1}) + (1 - \beta^*)b_{t-1}                \\
  s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + b_{t-1})} + (1 - \gamma)s_{t-m}.
\end{align*}

```{r}
# HOLT WINTERS multiplicativo
model_spec_hw_m <- exp_smoothing(
  error = "multiplicative",
  trend = "multiplicative",
  season = "multiplicative",
  seasonal_period = 12
) %>%
  set_engine("ets")

model_fit_hw_m  <- model_spec_hw_m  %>%
  fit(
    y ~ date,
    data = training(splits)
  )

model_fit_hw_m
```

```{r}
calibration_tbl <- model_fit_hw_m %>%
  # Calibracion en datos de test para evaluacion de rendimiento
  modeltime_calibrate(new_data = testing(splits)) %>%
  # Pronostico en datos de test
  modeltime_forecast(
    new_data    = testing(splits),
    actual_data = us_monthly
  )
calibration_tbl %>% rmarkdown::paged_table()

calibration_tbl %>% 
  # Grafico pronosticos
  plot_modeltime_forecast(.interactive = TRUE)
```

### Holt-Winters Amortiguado

Es posible amortig眉ar los m茅todos de Holt-Winters (aditivos y multiplicativos) revisados anteriormente. Un ejemplo es el m茅todo de Holt-Winters con tendencia amortiguada y estacionalidad multiplicativa que sigue:

\begin{align*}
  \hat{y}_{t+h|t} &= \left[\ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t}\right]s_{t+h-m(k+1)} \\
  \ell_{t} &= \alpha(y_{t} / s_{t-m}) + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1}             \\
  s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + \phi b_{t-1})} + (1 - \gamma)s_{t-m}.
\end{align*}

```{r}
# HOLT WINTERS multiplicativo con damping
model_spec_hw_m_damped <- exp_smoothing(
  error = "multiplicative",
  trend = "multiplicative",
  season = "multiplicative",
  damping = "damped",
  seasonal_period = 12
) %>%
  set_engine("ets")

model_fit_hw_m_damped  <- model_spec_hw_m_damped  %>%
  fit(
    y ~ date,
    data = training(splits)
  )

model_fit_hw_m_damped
```

```{r}
calibration_tbl <- model_fit_hw_m_damped %>%
  # Calibracion en datos de test para evaluacion de rendimiento
  modeltime_calibrate(new_data = testing(splits)) %>%
  # Pronostico en datos de test
  modeltime_forecast(
    new_data    = testing(splits),
    actual_data = us_monthly
  )
calibration_tbl %>% rmarkdown::paged_table()

calibration_tbl %>% 
  # Grafico pronosticos
  plot_modeltime_forecast(.interactive = TRUE)
```

## Modelos de espacio de estado (ETS)

Las combinaciones y variaciones de componentes de tendencia y estacionalidad hacen posible nueve m茅todos de suavizamiento exponencial, denotados por el par $(T,S)$ seg煤n el tipo de tendencia y estacionalidad (**T**rend, **S**easonality). As铆 el m茅todo $(A_d,M)$ es un suavizamiento exponencial con tendencia amortiguada y estacionalidad multiplicativa.

```{r TS, echo = FALSE, fig.cap = "Fuente: [A taxonomy of exponential smoothing methods (fpp3)](https://otexts.com/fpp3/taxonomy.html)"}
knitr::include_graphics("https://otexts.com/fpp3/figs/pegelstable-1.png")
```

Los m茅todos de suavizamiento exponencial poseen modelos estad铆sticos para generar intervalos de predicci贸n^[Adem谩s de la estimaci贸n puntual lograda con un suavizamiento exponencial.] y producir una distribuci贸n de pron贸stico. Cada modelo consiste de una *ecuaci贸n de medici贸n* (*measurement equation*) que describe los datos observados y algunas *ecuaciones de estado* (*state equations*) que describen la manera en que los componentes no observados o *estados* (nivel, tendencia, estacionalidad) cambian en el tiempo. De ah铆 que se conozcan como *modelos de espacio de estados* (*state space models*).

Para distinguir entre un modelo con errores aditivos de uno con errores multiplicativos, se agrega una tercera letra a la clasificaci贸n anterior, etiquetando a cada modelo de espacio de estados como ETS(Error, Trend, Seasonal). Podemos ver todos los modelos de espacio de estado en el siguiente cuadro:

```{r ETS, echo = FALSE, fig.cap = "Fuente: [Innovations state space models for exponential smoothing (fpp3)](https://otexts.com/fpp3/ets.html#ets)"}
knitr::include_graphics("https://otexts.com/fpp3/figs/statespacemodels-1.png")
```

Consideremos el suavizamiento exponencial simple con errores aditivos: ETS(A,N,N)

\begin{align*}
  \text{Ecuaci贸n de pron贸stico}  && \hat{y}_{t+1|t} & = \ell_{t}\\
  \text{Ecuaci贸n de suavizamiento} && \ell_{t}        & = \alpha y_{t} + (1 - \alpha)\ell_{t-1}.
\end{align*}

Si se reordena la ecuaci贸n de suavizamiento para el nivel, se obtiene la forma de correcci贸n de error que sigue:

\begin{align*}
\ell_{t} %&= \alpha y_{t}+\ell_{t-1}-\alpha\ell_{t-1}\\
         &= \ell_{t-1}+\alpha( y_{t}-\ell_{t-1})\\
         &= \ell_{t-1}+\alpha e_{t},
\end{align*}

donde $e_{t}=y_{t}-\ell_{t-1}=y_{t}-\hat{y}_{t|t-1}$ es el residual del per铆odo $t$. Por lo tanto, podemos escribir $y_t = \ell_{t-1}+ e_{t}$, tal que cada observaci贸n pueda ser representada por un nivel previo m谩s un error. Si se asume que los residuales (errores de entrenamiento a un paso) $e_t$ distribuyen como ruido blanco gaussiano, entonces las ecuaciones pueden escribirse:

\begin{align}
  y_t &= \ell_{t-1} + \varepsilon_t\\
  \ell_t&=\ell_{t-1}+\alpha \varepsilon_t
\end{align}

Donde la primera ecuaci贸n corresponde a la *ecuaci贸n de medici贸n* u observaci贸n y la segunda corresponde a la ecuaci贸n de *estado o transici贸n*. Estas ecuaciones, junto a la distribuci贸n estad铆stica del error, forman un modelo estad铆stico conocido como *modelo de espacio de estado de innovaciones* (*innovations state space model*) subyacente al modelo de suavizamiento exponencial simple asociado. Todas las ecuaciones utilizan las *innovaciones* provienen del mismo proceso de error aleatorio, $\varepsilon_t$, por lo que tambi茅n son conocidos como *modelos de fuente 煤nica de error*.

# Modelos ARIMA 

> Remember that all models are wrong; the practical question is how wrong do they have to be to not be useful^[Usualmente conocida como "All models are wrong, some are useful"] ^[Nota del autor: 隆siempre hab铆a querido poner la cita completa en alguna parte!ぉ] 
> 
>                     George E. P. Box

En la secci贸n anterior discutimos t茅cnicas de pron贸stico que, en t茅rminos generales, se basan en la idea de representar la serie de tiempo como una suma de componentes distintos: uno determin铆stico y otro estoc谩stico^[Aleatorio]. El componente determin铆stico se modela mediante una funci贸n del tiempo, mientras que para el componente estoc谩stico se asume un ruido blanco que es adicionado a la se帽al determin铆stica generando un comportamiento estoc谩stico en la serie de tiempo. Un supuesto importante es que el ruido aleatorio es generado por un proceso de elaboraci贸n de *innovaciones o shocks independientes*. Como se ha mencionado anteriormente, en la pr谩ctica dicho supuesto usualmente es violado, por lo que las observaciones sucesivas presentan dependencias seriales para las cuales un modelo de suavizamiento exponencial pueda ser ineficiente e incluso inapropiado^[Ya que podr铆an no aventajarse de la dependencia temporal en las observaciones de la manera m谩s efectiva]. Para incorporar de manera formal esta estructura de dependencia, a continuaci贸n exploraremos una clase general de modelos conocidos como modelos autoregresivos integrados de medias m贸viles (*Autoregressive Integrated Moving Average*) o ARIMA^[Tambi茅n hallados en la literatura como metodolog铆a de pron贸stico de Box-Jenkins].

## Diferenciaci贸n

En la [Clase 2](https://hapst.netlify.app/posts/clase-02) introdujimos el concepto de [*estacionariedad*](https://hapst.netlify.app/posts/clase-02/#estacionariedad) para describir una serie de tiempo cuyas propiedades estad铆sticas no dependen del momento en que la serie es observada. Por lo tanto, series con tendencia o estacionalidad no son estacionarias, pues la tendencia y la estacionalidad afectar铆an el valor de la serie de tiempo en diferentes momentos^[Por otro lado, recordemos que una serie de ruido blanco es estacionaria, no importa cuando se le observe, deber铆a verse similar en cualquier punto del tiempo]. Esto 煤ltimo puede ser confuso: una serie de tiempo puede exhibir un comportamiento c铆clico (aunque sin tendencia ni estacionalidad) y ser estacionaria, ya que los ciclos no poseen un largo fijo. 
En general una serie estacionaria no presentar谩 patrones predecibles al largo plazo. El gr谩fico deber铆a parecer una serie relativamente horizontal, con algunas variaciones c铆clicas y varianza constante.

```{r}
quantmod::getSymbols("GOOGL")
google_tbl <- GOOGL %>% 
  timetk::tk_tbl() %>% 
  filter(lubridate::year(index) == 2015)

google_tbl %>% 
  select(
    date = index,
    google_price = GOOGL.Close) %>% 
  timetk::plot_time_series(
    .date_var = date,
    .value = google_price
  )
```

En la figura anterior podemos ver el precio de cierre de las acciones de Apple, una serie claramente no estacionaria. Sin embargo, si calculamos la diferencia entre sus observaciones consecutivas podemos ver que la serie ahora es aparentemente estacionaria:

```{r}
google_tbl %>% 
  timetk::tk_tbl() %>% 
  select(
    date = index,
    google_price = GOOGL.Close) %>% 
  mutate(diff_google = timetk::diff_vec(google_price, lag = 1)) %>% 
  timetk::plot_time_series(
    .date_var = date,
    .value = diff_google
  )
```

Transformaciones tales como logar铆tmos (o Box-Cox) pueden ayudar a estabilizar la varianza de una serie de tiempo. En este caso, la diferenciaci贸n logra estabilizar la media de una serie de tiempo removiendo los cambios en el nivel de una serie, y por ende, eliminado (o reduciendo) su tendencia y estacionalidad. Tal como vimos en clases previas, el gr谩fico de la ACF es 煤til para identificar series de tiempo no estacionarias^[La ACF de una serie de tiempo estacoinaria decae a 0 relativamente r谩pido, mientras que para series no estacionarias lo hace lentamente].  

::::: {.panelset}

::: {.panel}
[Precio]{.panel-name}
```{r, echo = FALSE}
google_tbl %>% 
  timetk::tk_tbl() %>% 
  select(
    date = index,
    google_price = GOOGL.Close) %>% 
  timetk::plot_acf_diagnostics(
    .date_var = date,
    .value = google_price,
    .lags = 20,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Diferencia]{.panel-name}
```{r, echo = FALSE}
google_tbl %>%
  timetk::tk_tbl() %>%
  select(date = index,
         google_price = GOOGL.Close) %>%
  mutate(diff_google = timetk::diff_vec(google_price, lag = 1)) %>%
  timetk::plot_acf_diagnostics(
    .date_var = date,
    .value = diff_google,
    .lags = 20,
    .interactive = TRUE
  )
```
:::
:::::

La ACF de la diferencia de precios asociada a las acciones de Apple se ve id茅ntica la del ruido blanco. El test de Ljung-Box (`p-value = 0.6257`) sugiere que el cambio diario del precio de las acciones de Google es esencialmente un valor aleatorio que no est谩 correlacionado con d铆as previos.

```{r}
google_tbl %>%
  timetk::tk_tbl() %>%
  select(date = index,
         google_price = GOOGL.Close) %>%
  mutate(diff_google = timetk::diff_vec(google_price, lag = 1)) %>% 
  select(diff_google) %>%
  pull() %>% 
  Box.test(., lag = 10, type = "Ljung-Box")
```

## Caminata Aleatoria

La serie diferenciada es el cambio entre observaciones consecutivas de la serie original y puede ser escrito como:

$$y'_t = y_t - y_{t-1}$$ 

La serie diferenciada tendr谩 $T-1$ valores. Si dichos valores son ruido blanco, el modelo para la serie original puede ser escrito como

$$y_t - y_{t-1} = \varepsilon_t$$

donde $\varepsilon_t$ denota el ruido blanco. Reordenando, obtenemos el modelo de *caminata aleatoria* (*random walk*)   

$$y_t = y_{t-1} + \varepsilon_t$$

Las caminatas aleatorias son ampliamente utilizadas para datos no-estacionarios, particularmente financieros o econ贸micos, ya que t铆picamente poseen:

+ Per铆odos largos de tendencia al crecimiento/decrecimiento aparente
+ Cambios repentinos y poco predecibles en su direcci贸n

Los pron贸sticos asociados a las caminatas aleatorias son equivalentes a la 煤ltima observaci贸n^[隆Pron贸stico ingenuo!], dado que los movimientos futuros son impredecibles, con la misma probabilidad de ascender o descender.

En particular, si la diferencia no posee media 0, entonces

$$y_t - y_{t-1} = c + \varepsilon_t\quad\text{o}\quad {y_t = c + y_{t-1} + \varepsilon_t}$$

lo que se conoce como caminata aleatoria con deriva (*random walk with drift*).

```{r}
# Shumway & Stoffer example
set.seed(154)

w <- rnorm(200)
x <- cumsum(w)

wd <- w +.2 # drift
xd <- cumsum(wd)

rw_plot <- tibble(
  index = 1:length(w),
  x,
  xd
) %>% 
  pivot_longer(
    cols = -index,
    names_to = "series"
  ) %>% 
  ggplot(aes(x = index, y = value, color = series)) +
  geom_line() +
  geom_abline(intercept = 0, slope = 0, linetype="dotted") +
  geom_abline(intercept = 0, slope = 0.2, linetype="dotted") +
  labs(title = "Random Walk y Random Walk w/drift") + 
  theme_bw()

plotly::ggplotly(rw_plot)
```

El mismo principio de diferenciaci贸n aplica en series estacionales, donde la diferencia entra la observaci贸n y la observaci贸n previa de la misma temporada se conoce como *diferencia estacional*^[Hablando de la diferencia usual como *primera diferencia*]. En ocasiones ser谩 necesario aplicar diferencias estacionales y primeras diferencias para obtener datos estacionarios, existiendo un grado de subjetividad al momento de aplicar diferencias suficientes para obtener datos estacionarios. Debido a lo anterior, es imporante que las diferencias sean interpretables. Para determinar de manera m谩s objetiva si una serie requiere de diferencias o no es posible realizar el *test de ra铆z unitaria*^[Un test 煤til ser铆a el test de Kwiatkowski-Phillips-Schmidt-Shin (KPSS)].

## Modelos Autoregresivos (AR)

Un modelo autoregresivo de orden $p$, $AR(p)$, tiene la forma

$$y_{t} = c + \phi_{1}y_{t-1} + \phi_{2}y_{t-2} + \dots + \phi_{p}y_{t-p} + \varepsilon_{t}$$

e involucra la regresi贸n de una variable consigo misma. Aqu铆 $y_t$ es estacionaria y $\varepsilon_{t}$ es ruido blanco. Los par谩metros $\phi_{1}, \dots, \phi_{p}$ son constantes y la media de $y_t$ es cero^[Si la media de no es cero, entonces se reemplaza $y_t$ por $y_t - \mu$, donde $\mu$ es la media de la serie, en este caso $c$]. Note que para el modelo AR(1):

+ Si $\phi_1 = 0$ = 0 y $c = 0$, $y_t$ es ruido blanco
+ Si $\phi_1 = 1$ = 0 y $c = 0$, $y_t$ es una caminata aleatoria
+ Si $\phi_1 = 1$ = 0 y $c \neq 0$, $y_t$ es una caminata aleatoria con deriva
+ Si $\phi_1 < 0$, $y_t$ oscila alrededor de la media.

Usualmente los par谩metros $\phi$ y sus combinaciones lineales son restringidos a oscilar entre `[-1,1]`

## Modelos de Medias M贸viles (MA)

En el caso de las medias m贸viles, en lugar de utilizar valores pasados de la variable a pronosticar, se utilizan errores de pron贸stico para realizar un modelo del tipo regresi贸n. Nos referimos al modelo de medias m贸viles de orden $q$, $MA(q)$ con la expresi贸n

$$y_{t} = c + \varepsilon_t + \theta_{1}\varepsilon_{t-1} + \theta_{2}\varepsilon_{t-2} + \dots + \theta_{q}\varepsilon_{t-q}$$

donde $\varepsilon_{t}$ es ruido blanco. En este caso, cada valor de $y_t$ puede pensarse como una media m贸vil ponderada de los errores de pron贸stico pasados.

## Modelos Autoregresivos Integrados de Medias M贸viles (ARIMA)

Si se combinan los procesos de diferenciaci贸n con los modelos autoregresivos y de medias m贸viles, obtenemos el *modelo autoregresivo integrado de medias m贸viles* (ARIMA)^[No estacional]. En este contexto, con *integraci贸n* nos referimos al proceso inverso de la diferenciaci贸n. 

<aside>
ARIMA
</aside>

\begin{equation}
  y'_{t} = c + \phi_{1}y'_{t-1} + \cdots + \phi_{p}y'_{t-p}
     + \theta_{1}\varepsilon_{t-1} + \cdots + \theta_{q}\varepsilon_{t-q} + \varepsilon_{t}
\end{equation}

donde $y'_{t}$ es la serie diferenciada^[incluso m谩s de una vez]. El modelo $ARIMA(p,d,q)$ describe un modelo autoregresivo de orden $p$, con medias m贸viles de orden $q$ con grado de integraci贸n $d$.

Si definimos el operador de diferencias $B y_{t} = y_{t - 1}$, entonces $y'_{t} = y_{t} - y_{t-1} = y_t - By_{t} = (1 - B)y_{t}$ y $B(By_{t}) = B^{2}y_{t} = y_{t-2}$. De manera m谩s general, la diferencia de orden $d$ puede escribirse como $(1 - B)^{d} y_{t}$, luego:

\begin{equation}
  \begin{array}{c c c c}
    (1-\phi_1B - \cdots - \phi_p B^p) & (1-B)^d y_{t} &= &c + (1 + \theta_1 B + \cdots + \theta_q B^q)\varepsilon_t\\
    {\uparrow} & {\uparrow} & &{\uparrow}\\
    \text{AR($p$)} & \text{$d$ diferencias} & & \text{MA($q$)}\\
  \end{array}
\end{equation}

```{r}
# ARIMA
model_spec_arima <- arima_reg() %>%
  set_engine("auto_arima")

model_fit_arima <- model_spec_arima %>%
  fit(
    y ~ date,
    data = training(splits)
  )

model_fit_arima
```


```{r}
calibration_tbl <- model_fit_arima %>%
  # Calibracion en datos de test para evaluacion de rendimiento
  modeltime_calibrate(new_data = testing(splits)) %>%
  # Pronostico en datos de test
  modeltime_forecast(
    new_data    = testing(splits),
    actual_data = us_monthly
  )
calibration_tbl %>% rmarkdown::paged_table()

calibration_tbl %>% 
  # Grafico pronosticos
  plot_modeltime_forecast(.interactive = TRUE)
```

# Modelos de Regresi贸n Din谩mica 

# Modelos para estacionalidades complejas 

## tbats

Una aproximaci贸n alternativa a los modelos de regresi贸n din谩mica es el modelo `TBATS`^[**T**: estacionalidad trigonom茅trica, **B**: transformaci贸n Box-Cox, **A**: errores ARIMA, **T**: tendencia, **S**: componentes estacionales] 

```{r}
# TBATS
model_spec_tbats <- seasonal_reg() %>%
  set_engine("tbats")

model_fit_tbats <- model_spec_tbats %>%
  fit(
    y ~ date,
    data = training(splits)
  )

model_fit_tbats
```


## Modelo Prophet 

[`Prophet`](https://peerj.com/preprints/3190/) es un procedimiento de prontico de series de tiempo basado en modelos aditivos donde se ajustan tendencia no-lineals con estacionalidad anual, semanal y diaria, incorporando adem谩s efectos de calendario. Su potencial radica en el modelamiento de series con fuerte componente estacional y varias temporadas de datos hist贸ricos.

Prophet puede ser considerado un modelo de regresi贸n no lineal^[Similar a los Modelos Aditivos Generalizados (GAM)] de forma:

$$y_t = g(t) + s(t) + h(t) + \varepsilon_t$$

donde $g(t)$ describe una tendencia lineal por tramos (*piecewise-linear trend*), $s(t)$ describe los variados patrones estacionales, $h(t)$ captura los efectos de calendario y $\varepsilon_t$ es un t茅rmino de ruido blanco asociado al error. Es importante destacar que:

+ La cantidad de nodos (*knots*) o puntos de quiebre (*changepoints*) para la tendencia por tramos son seleccionados de manera autom谩tica si no son especificados.
+ La componente estacional consiste en t茅rminos de Fourier para los per铆odos relevantes, usando 10 para la estacionalidad anual y 3 para la semanal.
+ Los efectos de calendario son incorporados como variables dummy
+ El modelo es estimado usando una aproximaci贸n Bayesiana para la selecci贸n autom谩tica de par谩metros del modelo.

```{r}
# PROPHET
model_spec_prophet <- prophet_reg() %>%
  set_engine("prophet")

model_fit_prophet <- model_spec_prophet %>%
  fit(
    y ~ date,
    data = training(splits)
  )

model_fit_prophet
```

Finalmente, adicionamos la regresi贸n lineal pasada y el modelo estacional ingenuo como benchmarks y obtenemos:

```{r}
# LM
model_spec_lm <- linear_reg() %>%
  set_engine("lm")

model_fit_lm <- model_spec_lm %>%
  fit(
    y ~ as.numeric(date) + I(as.numeric(date) ^ 2) + lubridate::month(date, label = TRUE),
    data = training(splits)
  )

# SNAIVE

model_spec_snaive <- naive_reg(seasonal_period = 12) %>%
  set_engine("snaive")

model_fit_snaive <- model_spec_snaive %>%
  fit(y ~ date , data = training(splits))
model_fit_snaive
```

```{r}
# MODELTIME TABLE

models_tbl <- modeltime_table(
  model_fit_ses,
  model_fit_holt,
  model_fit_holt_damped,
  model_fit_hw,
  model_fit_hw_m,
  model_fit_hw_m_damped,
  model_fit_prophet,
  model_fit_tbats,
  model_fit_arima,
  model_fit_lm,
  model_fit_snaive
)
models_tbl

calibration_tbl <- models_tbl %>%
  # Calibracion en datos de test para evaluacion de rendimiento
  modeltime_calibrate(new_data = testing(splits)) %>%
  # Pronostico en datos de test
  modeltime_forecast(
    new_data    = testing(splits),
    actual_data = us_monthly
  )
calibration_tbl

calibration_tbl %>% 
  # Grafico pronosticos
  plot_modeltime_forecast(.interactive = TRUE)
```

```{r}
# Metricas de rendimiento
models_tbl %>%
  # Calibracion en datos de test para evaluacion de rendimiento
  modeltime_calibrate(new_data = testing(splits)) %>%
  modeltime_accuracy() %>%
  table_modeltime_accuracy(
    .interactive = TRUE
  )
```


---

```{r}
sessionInfo()
```


