---
title: "Clase 2"
description: |
  Series de Tiempo: Conceptos, An谩lisis, Manipulaci贸n y Visualizaci贸n
author:
  - name: Basti谩n Aballay L.
    url: https://www.linkedin.com/in/bastianaballay/
date: "2021-10-25"
bibliography: clase-02.bib
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: true
    highlight: tango
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r packages, include=FALSE}
suppressPackageStartupMessages({
  library(tidyverse)
  library(plotly)
  library(xaringanExtra)
})

xaringanExtra::use_panelset()
```

En la [Clase 1](posts/clase-01) introdujimos las series de tiempo de
manera conceptual como una colecci贸n de observaciones secuenciales en el tiempo.

En esta clase revisaremos las series de tiempo desde una perspectiva te贸rica,
estableciendo los antecendentes estad铆sticos necesarios para analizarlas y
caracterizarlas. Luego de esta clase podr谩s:

+ Entender las series de tiempo como *realizaciones* de procesos estoc谩sticos
+ Caracterizar series de tiempo utilizando medidas de dependencia y descomposiciones
+ Proponer visualizaciones adecuadas a la serie de tiempo estudiada

Por supuesto, todo lo anterior a trav茅s de la utilizaci贸n de distintos paquetes 
de `R`.

# Series de Tiempo 

Uno de los objetivos principales del an谩lisis de series de tiempo es desarrollar
modelos matem谩ticos que provean de descripciones plausibles para los datos
muestreados. Para establecer una configuraci贸n estad铆stica que nos permita describir
las series de tiempo, asumiremos que una serie de tiempo puede ser definida como
una colecci贸n de variables aleatorias indexadas de acuerdo al orden en que fueron
obtenidas a trav茅s del tiempo.

<aside>
Series de Tiempo
</aside>

Podemos entonces entender una serie de tiempo como la secuencia de variables aleatorias $y_1,
y_2,y_3,\dots$, donde la variable aleatoria $y_1$ denota el valor que toma la
serie en la primera observaci贸n temporal, la variable $y_2$ denota el valor que
toma el segundo punto en el tiempo y as铆 sucesivamente. Podemos referirnos a dicha secuencia de 
variables aleatorias $\{y_t\} = \{y_1,\dots,y_T\}$, indexada por $t$^[El 铆ndice $t$ usualmente es discreto y toma valores en los enteros] como un *proceso estoc谩stico*^[[Cross 
Validated: Is a time series the same as a stochastic process?](https://stats.stackexchange.com/questions/126791/is-a-time-series-the-same-as-a-stochastic-process)]. Luego, los valores observados de una serie de tiempo son una *realizaci贸n* del proceso estoc谩stico, siendo una de muchas posibles secuencias que un proceso aleatorio puede generar (@Shumway2017-dp).

<aside>
Proceso estoc谩stico
</aside>

Otra manera de ver a las series de tiempo es como una muestra finita que se obtiene de 
una secuencia doblemente infinita subyacente: $\{\dots, y_{-1},y_0,y_1,y_2,\dots,y_T,y_{T+1},y_{T+2},\dots\}$.

En este contexto, un *modelo de series de tiempo* para $\{y_t\}$  es una especificaci贸n
de las distribuciones conjuntas de la sencuencia de variables aleatorias para la cual
$\{y_t\}$ es una realizaci贸n. Dado lo anterior, un modelo de series de tiempo nos permitir谩
usar una realizaci贸n para realizar inferencias acerca de la distribuci贸n conjunta
subyacente desde donde se produjo la realizaci贸n obtenida.


```{r}
set.seed(42)

ts_tbl <-  replicate(10, rnorm(300, mean = 0, sd = 1)) %>%
  as_tibble() %>%
  rename_all( ~ paste0("ts_", 1:10)) %>%
  mutate(index = 1:300) %>% 
  pivot_longer(
    cols = -c(index),
    names_to = "series"
  ) %>% 
  mutate(realization = ifelse(series == "ts_1","Yes","No"))
```

::::: {.panelset}

::: {.panel}
[Distribuci贸n de Realizaciones]{.panel-name}
```{r ggplot_sim_ts_all, echo = FALSE}
ggplot_sim_ts_all <- ts_tbl %>% 
  ggplot(
    aes(x = index, y = value, color = series)
  ) +
  geom_line() +
  theme_bw()  

plotly::ggplotly(ggplot_sim_ts_all)

```
:::

::: {.panel}
[Realizaci贸n en particular]{.panel-name}
```{r ggplot_sim_ts_real}
ggplot_sim_ts_real <- ts_tbl %>% 
  ggplot(
    aes(x = index, y = value, color = realization)
  ) +
  geom_line() +
  scale_color_manual(values=c("lightgray","black")) +
  theme_bw()  
  
  plotly::ggplotly(ggplot_sim_ts_real)
```
:::
:::::

# Componentes de una Serie de Tiempo

En la [Clase 1](posts/clase-01) describimos al an谩lisis exploratorio como uno de los objetivos
principales del an谩lisis de series de tiempo, haciendo 茅nfasis en la necesidad de 
identificar dichos fen贸menos para poder caracterizar las series estudiadas. 
A continuaci贸n definimos de manera m谩s formal a qu茅 nos referiremos cuando 
hablemos de tendencia, estacionalidad y ciclos, entre otros t茅rminos (@Hyndman2021-hc).

## Patrones regulares

### Tendencia

Hablamos de que existe *tendencia* en una serie de tiempo cuando hay un patr贸n
creciente o decreciente a largo plazo^[驴Qu茅 es largo plazo de todas maneras?
Para el clima una variaci贸n c铆clica puede ocurrir en un per铆odo de 50 a帽os. Si
s贸lo tuvieramos 20 a帽os de datos, esta oscilaci贸n podr铆a parecer una tendencia...]
en los datos. Cuando la tendencia cambia de una tendencia creciente a decreciente 
hablamos de un *cambio en la direcci贸n* de la serie. Al analizar la tendencia, es 
necesario considerar la cantidad de observaciones disponible y realizar una
evaluaci贸n subjetiva de a qu茅 nos referimos con *largo plazo*. Existen
m茅todos para estimar o remover la tendencia y as铆 observar de mejor manera
las otras fuentes de variaci贸n de una serie. Finalmente, si la serie no posee tendencia podemos considerarla *estacionaria*^[Retomaremos este concepto de manera m谩s formal un poco m谩s
adelante] en la media.

<aside>
<p>Tendencia</p>
<p>Estacionariedad</p>
</aside>

```{r}
# Global mean land-ocean temperature deviations (from 1951-1980 average), measured in degrees centigrade, for the years 1880-2015.
ggplot_globtemp <-
  astsa::globtemp %>% 
  timetk::tk_tbl() %>% 
  ggplot(aes(x = index, y = value)) +
  geom_line()

plotly::ggplotly(ggplot_globtemp)
```

### Estacionalidad

Un patr贸n estacional ocurre cuando una serie de tiempo es afectada por factores
estacionales tales como el per铆odo del a帽o o el d铆a de la semana. Es decir,
hablamos de una serie con *estacionalidad*  cuando la serie muestra variaciones
sistem谩ticas a trav茅s de un per铆odo de tiempo determinado. La estacionalidad
en general es fija y de per铆odo conocido. Este tipo de variaci贸n es f谩cil de 
entender y puede ser facilmente estimado si el efecto estacional es de inter茅s directo.
De manera alternativa, tambi茅n es posible remover la variaci贸n estacional de los
datos, para obtener datos *desestacionalizados*, si dicha variaci贸n no es de inter茅s.

<aside>
Estacionalidad
</aside>

```{r}
ggplot_airpassengers <- AirPassengers %>%
  tsibble::as_tsibble() %>%
  mutate(index = as.Date(index)) %>% 
  ggplot((aes(x = index, y = value))) +
  geom_line() +
  theme_bw()

plotly::ggplotly(ggplot_airpassengers)
```


### Ciclicidad

Un *ciclo* ocurre cuando los datos exhiben alzas y bajas que no poseen frecuencia
determinada. Estas fluctuaciones continuas en la tendencia se asocian usualmente
a condiciones econ贸micas determinadas por el tipo de industria o negocio estudiado.
La duraci贸n usual de estas fluctuaciones es de al menos 2 a帽os.

> Si las fluctuaciones presentes en una serie de tiempo no poseen una frecuencia
determinada son consideradas c铆clicas. Si la frecuencia es invariante y asociada
a alg煤n aspecto del calendario, entonces el patr贸n es estacional.

## Patrones irregulares

### Irregularidades

Luego de que las variaciones asociadas a patrones de tendencia o estacionalidad
han sido removidos de un set de datos, obtenemos una serie de *residuales*^[Retomaremos
este concepto de manera m谩s formal un poco m谩s adelante]
que pueden parecer (o no) aleatorios. Los elementos irregulares de una serie de 
tiempo le dan sus caracter铆sticas no-sistem谩ticas. Al hacer pron贸sticos, la
idea es *calibrar* cada uno de los componentes de una serie de tiempo en una 
manera precisas excepto por el componente irregular.

<aside>
Residual
</aside>

### Outliers

Desde una perspectiva tradicional, una anomal铆a u *outlier* es una observaci贸n
que se desv铆a con respecto a las otras observaciones lo suficiente como para
generar sospechas acerca de su proceso de generaci贸n. Es decir, un outlier
es una observaciones que no sigue un comportamiento esperado. Si la observaci贸n
es indeseada (por ejemplo un error de medici贸n producto de un sensor
descalibrado o evento de ocurrencia 煤nica en el calendario), usualmente podemos 
limpiarla o imputarla. Sin embargo, si el evento es de inter茅s, quiz谩 sea necesario
analizar el outlier de manera aislada (por ejemplo en detecci贸n de fraude).

<aside>
Outliers
</aside>

```{r ggplot_btc_usd_anoms, fig.show = 'hide'}
# devtools::install_github("amrrs/coindeskr")
library(coindeskr)

# Obtenemos precio historico del Bitcoin en USd麓
btc_usd_tbl <-
  get_historic_price('USD', '2019-01-01', '2021-10-25') %>%
  timetk::tk_tbl() %>%
  rename(Date = index)

# devtools::install_github("twitter/AnomalyDetection")
# Deteccion de anomalias
library(AnomalyDetection)
btc_usd_anoms <-
  AnomalyDetectionTs(
    btc_usd_tbl,
    max_anoms = 0.05,
    direction = 'both',
    plot = FALSE
  )

# Graficamos
ggplot_btc_usd_anoms <- btc_usd_anoms$anoms %>%
  as_tibble() %>%
  mutate(Date = as.Date(timestamp), .keep = "unused") %>%
  left_join(btc_usd_tbl, .) %>%
  ggplot(aes(x = Date, y = Price)) +
  geom_line() +
  geom_point(aes(y = anoms), color = 'red') +
  theme_bw()

plotly::ggplotly(ggplot_btc_usd_anoms)
```


### Cambios estructurales

Un cambio estructural (a veces llamados cambios de r茅gimen) es un cambio repentino e inesperado en el comportamiento de una serie de tiempo. En t茅rminos estad铆sticos, un cambio estructural ocurre cuando la distribuci贸n de probabilidad subyacente de una serie de tiempo cambia. El proceso de detecci贸n de puntos de cambios^[V茅ase paquete [`changepoint`](https://www.lancs.ac.uk/~killick/Pub/KillickEckley2011.pdf)] busca identificar cuando ocurren estos cambios, usualmente utilizando
algoritmos que comparan propiedades estad铆sticas de la distribuci贸n nueva con respecto a la original.

```{r}
library(changepoint)

set.seed(42)

# Simulamos a partir de una distribucion normal
ts_sim <-
  c(rnorm(100, mean = 0, sd = 1),
    rnorm(100, mean = 1, sd = 1),
    rnorm(100, mean = 0, sd = 1),
    rnorm(100, mean = 0.2, sd = 1))

# Calculamos posicionamiento optimo y cantidad (potencial) de
# puntos de cambio en los datos usando PELT
ts_pelt <- cpt.mean(ts_sim, method='PELT')

# [!] Notar que ts_pelt es un objeto clase S4
# Establecemos la media para cada intervalo hallado
ts_levels <-
  rep(ts_pelt@param.est$mean, 
      times = c(ts_pelt@cpts[1], diff(ts_pelt@cpts)))

# Generamos tibble que reune resultados
ts_cpoint_tbl <-
  dplyr::tibble(index = 1:400,
                ts = ts_sim,
                ts_level = ts_levels)

# Grafico
ggplot_ts_cpoint <- ts_cpoint_tbl %>%
  ggplot(aes(x = index, y = ts_sim)) +
  geom_line() +
  geom_line(y = ts_levels, color = 'red') +
  theme_bw()

plotly::ggplotly(ggplot_ts_cpoint)
```

# Descomposici贸n de Series de Tiempo

Hemos evidenciado que las series de tiempo pueden exhibir un sinn煤mero de patrones, 
tales como tendencia, estacionalidad y ciclos, por mencionar algunos. Cuando 
*descomponemos* series de tiempo, lo hacemos mediante la combinaci贸n de la tendencia
y ciclicidad en un s贸lo componente tendencia-ciclidad^[En general s贸lo "tendencia".],
un componente estacional (pudiendo existir m谩s de una componente estacional o ninguna) y un componente residual, que contiene la informaci贸n restante de la serie de tiempo. 

```{r}
timetk::m4_hourly %>%
  filter(id == "H10") %>%
  timetk::plot_stl_diagnostics(
    date, value,
    .feature_set = c("observed", "season", "trend", "remainder"),
    .frequency   = "24 hours",
    .trend       = "1 week",
    .interactive = TRUE)
```

La extracci贸n de componentes a partir de una serie de tiempo permite no s贸lo mejorar
el entendimiento de una serie de tiempo, sino tambi茅n ser utilizado para mejorar la 
elaboraci贸n de pron贸sticos

En el ejemplo anterior^[(https://business-science.github.io/timetk/reference/plot_stl_diagnostics.html)] pudimos ver la aplicaci贸n de la *descomposici贸n en tendencia y estacionalidad por LOESS ([locally estimated scatterplot smoothing](https://en.wikipedia.org/wiki/Local_regression))*^[ Es bueno conocer el trabajo de B.D. Ripley.] (STL), que permite descomponer de manera aditiva^[por defecto, si no se aplican transformaciones tipo Box-Cox] los componentes. La regresi贸n local
es de utilidad ya que nos permite aplicar un suavizador no-param茅trico realizando
ajustes por m铆nimos cuadrados en vecindades de una serie num茅rica^[ Es posible [optimizar sus hiper-par谩metros (i.e `span`)](http://r-statistics.co/Loess-Regression-With-R.html)]. 

# Medidas de dependencia

## Autocorrelaci贸n

As铆 como la correlaci贸n mide el grado de relaci贸n lineal entre dos variables,
la *autocorrelaci贸n*^[a.k.a. Correlaci贸n Serial] mide la relaci贸n lineal entre valores rezagados de una serie de tiempo.

<aside>
Autocorrelaci贸n
</aside>

Existen varios coeficientes de autocorrelaci贸n, cada uno correspondiente a cada
panel obtenido en un *lag plot*, donde se muestra $y_t$ con respecto a 
$y_{t-k}$ para diferentes valores de $k$ como sigue:

```{r ggplot_airpassengers_lag}
ggplot_airpassengers_lag <- AirPassengers %>%
  tsibble::as_tsibble() %>%
  feasts::gg_lag(value,geom = "point",lags = 1:12) +
  theme_bw()

plotly::ggplotly(ggplot_airpassengers_lag)
```

Aqu铆 los colores indican el mes de la vairable en el eje vertical, mientras
que los rezagos est谩n graficados en el eje horizontal.

Si $r_k$ mide la relaci贸n entre $y_t$ y $y_{t-k}$, entonces la autocorrelaci贸n
del rezago $k$,$r_k$, puede escribirse como:

$$r_{k} = \frac{\sum\limits_{t=k+1}^T (y_{t}-\bar{y})(y_{t-k}-\bar{y})}
 {\sum\limits_{t=1}^T (y_{t}-\bar{y})^2}$$

donde $T$ es el largo de la serie de tiempo. Los coeficientes de autocorrelaci贸n
consolidan la *funci贸n de autocorrelaci贸n* (ACF) de las serie de tiempo.

Revisemos las ACF de las series de tiempo vistas la [clase pasada](posts/clase-01):

::::: {.panelset .sideways}

::: {.panel}
[Nile]{.panel-name}
```{r acf_nile, echo = FALSE}
# Yearly
Nile %>% 
  tsibble::as_tsibble() %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 20,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[AirPassengers]{.panel-name}
```{r acf_airpassengers}
# Quarterly
AirPassengers %>%
  tsibble::as_tsibble() %>%
  mutate(index = as.Date(index)) %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 12,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Taylor]{.panel-name}
```{r acf_taylor}
# Half-hourly
forecast::taylor %>% 
  tsibble::as_tsibble() %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 48*14,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[EuStockMarkets]{.panel-name}
```{r acf_eu_stock_markets}
# Daily
# DAX: Germany
# SMI: Switzerland
# CAC: France
# FTSE: UK
EuStockMarkets %>%
  timetk::tk_tbl() %>%
  select(DAX) %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = DAX,
    .lags = 30,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[M4-q10]{.panel-name}
```{r acf_q10_quarterly}
# Quarterly
timetk::m4_quarterly %>% filter(id == "Q10") %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 12,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Sunspots]{.panel-name}
```{r acf_sunspots}
# Monthly
datasets::sunspots %>%
  timetk::tk_tbl() %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 12,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Wine]{.panel-name}
```{r acf_wine}
# Monthly
forecast::wineind %>%
  timetk::tk_tbl() %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 24,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Global Temperature]{.panel-name}
```{r acf_gtemp}
# Yearly average global temperature deviations
astsa::gtemp_land %>% 
  timetk::tk_tbl() %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 10,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Speech]{.panel-name}
```{r acf_speech}
# Speech recording of the syllable aaahhh sampled at 10,000 points per second
astsa::speech %>%
  timetk::tk_tbl() %>%
  mutate(index = 1:length(astsa::speech)) %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 200,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Earthquake]{.panel-name}
```{r ggplot_quake}
# 40 points per second
astsa::eqexp$EQ5 %>%
  timetk::tk_tbl() %>%
  mutate(index = 1:length(astsa::eqexp$EQ5)) %>%
  timetk::plot_acf_diagnostics(.date_var = index,
                               .value = data,
                               .lags = 40*10,
                               .interactive = TRUE
  )
```
:::
:::::

Usualmente graficamos la ACF para ver c贸mo las correlaciones var铆an con respecto
al lag $k$-茅simo. En la literatura el gr谩fico de la ACF es conocido como *correlograma*.

Al analizar una ACF nos interesar谩 enfocarnos en los siguientes fen贸menos^[M谩s adelante
ahondaremos en los modelos que se podr铆an utilizar a partir del an谩lisis de una ACF]:

+ $r_k$ es m谩s alto para un $k$ en particular con respecto a los dem谩s. Usualmente
asociado a patrones estacionales en los datos.
+ Autocorrelaciones altas y positivas y lentamente decrecientes o *persistentes* (@Montgomery2015-fr) asociadas a tendencia en la serie de tiempo, usualmente por la cercan铆a en el tiempo de las observaciones.
+ Dependiendo del paquete de `R`, el correlograma puede incluir intervalos de confianza
dibujados en forma de cono, usualmente asociados al 90% ($\pm 1.96/\sqrt{T}$) o 95%, sugiriendo que valores de correlaci贸n fuera de este intervalo se asocian con alta probabilidad a correlaci贸n y no a casualidad estad铆stica.

## Autocorrelaci贸n Parcial

Las autocorrelaciones entre rezagos est谩n formadas por los efectos de correlaci贸n
directos e indirectos. La *autocorrelaci贸n parcial* (PACF) para el rezago $k$ es la correlaci贸n que resulta luego de remover el efecto de cualquier correlaci贸n asociada a los t茅rminos de rezagos intermedios (@Cowpertwait2009-qv), permitiendo
aislar los efectos directos de cada rezago $k$ en los valores actuales de la serie analizada.

# Modelos de Series de Tiempo 

El grado de suavidad de las series de tiempo revisadas en la [clase pasada](posts/clase-01/index.html#introST) es una de las caracter铆sticas
fundamentales que nos permite diferenciarlas entre ellas. Como vimos anteriormente,
podr铆amos suponer que los puntos adyacentes en el tiempo est谩n *correlacionados*,
por lo que el valor de la serie en el per铆odo $t$, $y_t$, depender铆a de alg煤n modo
de sus valores pasados $y_{t-1}, y_{t-2},\dots$. Podemos incorporar dicha suposici贸n
junto a colecciones de variables aleatorias para modelar series de tiempo.

A continuaci贸n revisaremos la serie m谩s simple que podemos generar: una colecci贸n
de variables aleatorias no correlacionadas utilizando la distribuci贸n normal.

## Ruido Blanco

El *ruido blanco*^[En ingl茅s White Noise (WN), en analog铆a con la luz blanca y la idea
de que todas las oscilaciones peri贸dicas posibles est谩n presentes en ella con igual fuerza.]
es una de las series de tiempo m谩s simples que podemos generar y es ampliamente
utilizada como modelo de ruido en aplicaciones ingenieriles. Corresponde a 
observaciones aleatorias, independientes e identicamente distribuidas, lo que los
estad铆sticos llaman variables aleatorias *iid* (*independent and identically distributed*).

<aside>
Ruido Blanco (iid)
</aside>

Un ruido blanco particularmente 煤til es el *ruido blanco Gaussiano*, donde
$w_t$ son variables aleatorias independientes con media 0 y varianza $\sigma_{w}^{2}$,
lo que podemos escribir como $w_t \sim \text{iid}\ N(0,\sigma_{w}^{2})$.

Anteriormente utilizamos `rnorm()` para generar posibles realizaciones de una serie de
tiempo. Dicha serie no pose铆a tendencia, estacionalidad ni ciclicidades aparentes, por
lo que vale la pena preguntarse *驴c贸mo ser谩 su ACF?* 

```{r}
ts_tbl %>% 
  filter(series == "ts_1") %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 100,
    .interactive = TRUE
  )

```

Podemos ver que la serie no posee ning煤n tipo de correlaci贸n, s贸lo ruido. No existe
informaci贸n para poder construir un modelo de pron贸stico. Para evaluar nuestra hip贸tesis,
podemos utilizar el test de Ljung-Box (`Box.test()`)^[[https://koalatea.io/r-ljung-box-test/](https://koalatea.io/r-ljung-box-test/)], que considera la autocorrelaci贸n de los primeros $h$ valores juntos. La significancia del test indica que los datos probablemente no son
ruido blanco.

## Estacionariedad

El ruido blanco es el ejemplo m谩s simple de un *proceso estacionario*. Una serie de tiempo
se dice *estrictamente estacionaria* si sus propiedades no se ven afectadas un cambio
en el origen del tiempo. Es decir, la distribuci贸n de probabilidad conjunta de las observaciones
$y_t, y_{t+1}, \dots, y_{t+n}$ es exactamente la misma que la distribuci贸n de probabilidad
conjunta de $y_t+k, y_{t+k+1}, \dots, y_{t+k+n}$. 

<aside>
Estacionariedad
</aside>

La estacionariedad implica un tipo de *equilibrio* o *estabilidad* estad铆stica en los datos. Por ello, la serie de tiempo posee una media constante definida de manera usual

$$\mu_y = E(y) = \int_{-\infty}^{\infty}yf(y)dy$$
y varianza constante definida como

$$\sigma_{y}^{2} = \text{Var}(y) = \int_{-\infty}^{\infty}(y - \mu_{y}^{})^{2}f(y)dy$$

La media y varianza muestral pueden ser utilizadas para estimar dichos par谩metros. Si 
las observaciones en una serie de tiempo son $y_1, y_2, \dots, y_{T}$, entonces la media
muestral es 

$$\bar{y} = \hat{\mu}_{y} = \frac{1}{T}\sum_{t=1}^{T}y_t$$
y la varianza muestral^[No hay mucha diferencia entre usar $T$ y $T-1$ cuando se tienen grandes candidades de observaciones] es 

$$s^{2} = \hat{\sigma}_{y}^{2} = \frac{1}{T}\sum_{t=1}^{T}(y_t - \bar{y})^{2}$$

### Estacionariedad d茅bil

Para que la ACF tenga sentido, la serie debe ser considerada una serie *debilmente estacionaria*. Esto implica que la funci贸n de autocorrelaci贸n para cualquier rezago
particular es la misma sin importar el lugar en el que estamos en el tiempo. La serie
ser谩 debilmente estacionaria si 

+ La media $E(y_t)$ es la misma para todo $t$
+ La varianza de $y_t$ es la misma para todo $t$.
+ La covarianza (y la correlaci贸n) entre $y_t$ y $y_{t-k}$ es la misma
para todo $t$ en cada rezago $k = 1,2,3 \dots$

---

```{r}
sessionInfo()
```
