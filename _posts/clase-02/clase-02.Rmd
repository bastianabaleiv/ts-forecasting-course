---
title: "Clase 2"
description: |
  Series de Tiempo: Conceptos, An√°lisis, Manipulaci√≥n y Visualizaci√≥n
author:
  - name: Basti√°n Aballay L.
    url: https://www.linkedin.com/in/bastianaballay/
date: "2022-01-17"
bibliography: clase-02.bib
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: true
    highlight: tango
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r packages, include=FALSE}
suppressPackageStartupMessages({
  library(tidyverse)
  library(plotly)
  library(xaringanExtra)
})

xaringanExtra::use_panelset()
```

En la [Clase 1](https://hapst.netlify.app/posts/clase-01/#introST) introdujimos las series de tiempo de
manera conceptual como una colecci√≥n de observaciones secuenciales en el tiempo.

En esta clase revisaremos las series de tiempo desde una perspectiva te√≥rica,
estableciendo los antecendentes estad√≠sticos necesarios para analizarlas y
caracterizarlas. Luego de esta clase podr√°s:

+ Entender las series de tiempo como *realizaciones* de procesos estoc√°sticos
+ Caracterizar series de tiempo utilizando medidas de dependencia y descomposiciones
+ Proponer visualizaciones adecuadas a la serie de tiempo estudiada

Por supuesto, todo lo anterior a trav√©s de la utilizaci√≥n de distintos paquetes 
de `R`.

# Series de Tiempo üìà

Uno de los objetivos principales del an√°lisis de series de tiempo es desarrollar
modelos matem√°ticos que provean de descripciones plausibles para los datos
muestreados. Para establecer una configuraci√≥n estad√≠stica que nos permita describir
las series de tiempo, asumiremos que una serie de tiempo puede ser definida como
una colecci√≥n de variables aleatorias indexadas de acuerdo al orden en que fueron
obtenidas a trav√©s del tiempo.

<aside>
Series de Tiempo
</aside>

Podemos entonces entender una serie de tiempo como la secuencia de variables aleatorias $y_1,
y_2,y_3,\dots$, donde la variable aleatoria $y_1$ denota el valor que toma la
serie en la primera observaci√≥n temporal, la variable $y_2$ denota el valor que
toma el segundo punto en el tiempo y as√≠ sucesivamente. Podemos referirnos a dicha secuencia de 
variables aleatorias $\{y_t\} = \{y_1,\dots,y_T\}$, indexada por $t$^[El √≠ndice $t$ usualmente es discreto y toma valores en los enteros] como un *proceso estoc√°stico*^[[Cross 
Validated: Is a time series the same as a stochastic process?](https://stats.stackexchange.com/questions/126791/is-a-time-series-the-same-as-a-stochastic-process)]. Luego, los valores observados de una serie de tiempo son una *realizaci√≥n* del proceso estoc√°stico, siendo una de muchas posibles secuencias que un proceso aleatorio puede generar (@Shumway2017-dp).

<aside>
Proceso estoc√°stico
</aside>

Otra manera de ver a las series de tiempo es como una muestra finita que se obtiene de 
una secuencia doblemente infinita subyacente: $\{\dots, y_{-1},y_0,y_1,y_2,\dots,y_T,y_{T+1},y_{T+2},\dots\}$.

En este contexto, un *modelo de series de tiempo* para $\{y_t\}$  es una especificaci√≥n
de las distribuciones conjuntas de la sencuencia de variables aleatorias para la cual
$\{y_t\}$ es una realizaci√≥n. Dado lo anterior, un modelo de series de tiempo nos permitir√°
usar una realizaci√≥n para realizar inferencias acerca de la distribuci√≥n conjunta
subyacente desde donde se produjo la realizaci√≥n obtenida.


```{r}
set.seed(42)

ts_tbl <-  replicate(10, rnorm(300, mean = 0, sd = 1)) %>%
  as_tibble() %>%
  rename_all( ~ paste0("ts_", 1:10)) %>%
  mutate(index = 1:300) %>% 
  pivot_longer(
    cols = -c(index),
    names_to = "series"
  ) %>% 
  mutate(realization = ifelse(series == "ts_1","Yes","No"))
```

::::: {.panelset}

::: {.panel}
[Distribuci√≥n de Realizaciones]{.panel-name}
```{r ggplot_sim_ts_all, echo = FALSE}
ggplot_sim_ts_all <- ts_tbl %>% 
  ggplot(
    aes(x = index, y = value, color = series)
  ) +
  geom_line() +
  theme_bw()  

plotly::ggplotly(ggplot_sim_ts_all)

```
:::

::: {.panel}
[Realizaci√≥n en particular]{.panel-name}
```{r ggplot_sim_ts_real}
ggplot_sim_ts_real <- ts_tbl %>% 
  ggplot(
    aes(x = index, y = value, color = realization)
  ) +
  geom_line() +
  scale_color_manual(values=c("lightgray","black")) +
  theme_bw()  
  
  plotly::ggplotly(ggplot_sim_ts_real)
```
:::
:::::

# Componentes de una Serie de Tiempo

En la [Clase 1](https://hapst.netlify.app/posts/clase-01/) describimos al an√°lisis exploratorio como uno de los objetivos
principales del an√°lisis de series de tiempo, haciendo √©nfasis en la necesidad de 
identificar dichos fen√≥menos para poder caracterizar las series estudiadas. 
A continuaci√≥n definimos de manera m√°s formal a qu√© nos referiremos cuando 
hablemos de tendencia, estacionalidad y ciclos, entre otros t√©rminos (@Hyndman2021-hc).

## Patrones regulares

### Tendencia

Hablamos de que existe *tendencia* en una serie de tiempo cuando hay un patr√≥n
creciente o decreciente a largo plazo^[¬øQu√© es largo plazo de todas maneras?
Para el clima una variaci√≥n c√≠clica puede ocurrir en un per√≠odo de 50 a√±os. Si
s√≥lo tuvieramos 20 a√±os de datos, esta oscilaci√≥n podr√≠a parecer una tendencia...]
en los datos. Cuando la tendencia cambia de una tendencia creciente a decreciente 
hablamos de un *cambio en la direcci√≥n* de la serie. Al analizar la tendencia, es 
necesario considerar la cantidad de observaciones disponible y realizar una
evaluaci√≥n subjetiva de a qu√© nos referimos con *largo plazo*. Existen
m√©todos para estimar o remover la tendencia y as√≠ observar de mejor manera
las otras fuentes de variaci√≥n de una serie. Finalmente, si la serie no posee tendencia podemos considerarla *estacionaria*^[Retomaremos este concepto de manera m√°s formal un poco m√°s
adelante] en la media.

<aside>
<p>Tendencia</p>
<p>Estacionariedad</p>
</aside>

```{r}
# Global mean land-ocean temperature deviations (from 1951-1980 average), measured in degrees centigrade, for the years 1880-2015.
ggplot_globtemp <-
  astsa::globtemp %>% 
  timetk::tk_tbl() %>% 
  ggplot(aes(x = index, y = value)) +
  geom_line()

plotly::ggplotly(ggplot_globtemp)
```

### Estacionalidad

Un patr√≥n estacional ocurre cuando una serie de tiempo es afectada por factores
estacionales tales como el per√≠odo del a√±o o el d√≠a de la semana. Es decir,
hablamos de una serie con *estacionalidad*  cuando la serie muestra variaciones
sistem√°ticas a trav√©s de un per√≠odo de tiempo determinado. La estacionalidad
en general es fija y de per√≠odo conocido. Este tipo de variaci√≥n es f√°cil de 
entender y puede ser facilmente estimado si el efecto estacional es de inter√©s directo.
De manera alternativa, tambi√©n es posible remover la variaci√≥n estacional de los
datos, para obtener datos *desestacionalizados*, si dicha variaci√≥n no es de inter√©s.

<aside>
Estacionalidad
</aside>

```{r}
ggplot_airpassengers <- AirPassengers %>%
  tsibble::as_tsibble() %>%
  mutate(index = as.Date(index)) %>% 
  ggplot((aes(x = index, y = value))) +
  geom_line() +
  theme_bw()

plotly::ggplotly(ggplot_airpassengers)
```


### Ciclicidad

Un *ciclo* ocurre cuando los datos exhiben alzas y bajas que no poseen frecuencia
determinada. Estas fluctuaciones continuas en la tendencia se asocian usualmente
a condiciones econ√≥micas determinadas por el tipo de industria o negocio estudiado.
La duraci√≥n usual de estas fluctuaciones es de al menos 2 a√±os.

> Si las fluctuaciones presentes en una serie de tiempo no poseen una frecuencia
determinada son consideradas c√≠clicas. Si la frecuencia es invariante y asociada
a alg√∫n aspecto del calendario, entonces el patr√≥n es estacional.

## Patrones irregulares

### Irregularidades

Luego de que las variaciones asociadas a patrones de tendencia o estacionalidad
han sido removidos de un set de datos, obtenemos una serie de *residuales*^[Retomaremos
este concepto de manera m√°s formal un poco m√°s adelante]
que pueden parecer (o no) aleatorios. Los elementos irregulares de una serie de 
tiempo le dan sus caracter√≠sticas no-sistem√°ticas. Al hacer pron√≥sticos, la
idea es *calibrar* cada uno de los componentes de una serie de tiempo en una 
manera precisas excepto por el componente irregular.

<aside>
Residual
</aside>

### Outliers

Desde una perspectiva tradicional, una anomal√≠a u *outlier* es una observaci√≥n
que se desv√≠a con respecto a las otras observaciones lo suficiente como para
generar sospechas acerca de su proceso de generaci√≥n. Es decir, un outlier
es una observaciones que no sigue un comportamiento esperado. Si la observaci√≥n
es indeseada (por ejemplo un error de medici√≥n producto de un sensor
descalibrado o evento de ocurrencia √∫nica en el calendario), usualmente podemos 
limpiarla o imputarla. Sin embargo, si el evento es de inter√©s, quiz√° sea necesario
analizar el outlier de manera aislada (por ejemplo en detecci√≥n de fraude).

<aside>
Outliers
</aside>

```{r ggplot_btc_usd_anoms, fig.show = 'hide'}
# devtools::install_github("amrrs/coindeskr")
library(coindeskr)

# Obtenemos precio historico del Bitcoin en USd¬¥
btc_usd_tbl <-
  get_historic_price('USD', '2019-01-01', '2021-10-25') %>%
  timetk::tk_tbl() %>%
  rename(Date = index)

# devtools::install_github("twitter/AnomalyDetection")
# Deteccion de anomalias
library(AnomalyDetection)
btc_usd_anoms <-
  AnomalyDetectionTs(
    btc_usd_tbl,
    max_anoms = 0.05,
    direction = 'both',
    plot = FALSE
  )

# Graficamos
ggplot_btc_usd_anoms <- btc_usd_anoms$anoms %>%
  as_tibble() %>%
  mutate(Date = as.Date(timestamp), .keep = "unused") %>%
  left_join(btc_usd_tbl, .) %>%
  ggplot(aes(x = Date, y = Price)) +
  geom_line() +
  geom_point(aes(y = anoms), color = 'red') +
  theme_bw()

plotly::ggplotly(ggplot_btc_usd_anoms)
```


### Cambios estructurales

Un cambio estructural (a veces llamados cambios de r√©gimen) es un cambio repentino e inesperado en el comportamiento de una serie de tiempo. En t√©rminos estad√≠sticos, un cambio estructural ocurre cuando la distribuci√≥n de probabilidad subyacente de una serie de tiempo cambia. El proceso de detecci√≥n de puntos de cambios^[V√©ase paquete [`changepoint`](https://www.lancs.ac.uk/~killick/Pub/KillickEckley2011.pdf)] busca identificar cuando ocurren estos cambios, usualmente utilizando
algoritmos que comparan propiedades estad√≠sticas de la distribuci√≥n nueva con respecto a la original.

```{r}
library(changepoint)

set.seed(42)

# Simulamos a partir de una distribucion normal
ts_sim <-
  c(rnorm(100, mean = 0, sd = 1),
    rnorm(100, mean = 1, sd = 1),
    rnorm(100, mean = 0, sd = 1),
    rnorm(100, mean = 0.2, sd = 1))

# Calculamos posicionamiento optimo y cantidad (potencial) de
# puntos de cambio en los datos usando PELT
ts_pelt <- cpt.mean(ts_sim, method='PELT')

# [!] Notar que ts_pelt es un objeto clase S4
# Establecemos la media para cada intervalo hallado
ts_levels <-
  rep(ts_pelt@param.est$mean, 
      times = c(ts_pelt@cpts[1], diff(ts_pelt@cpts)))

# Generamos tibble que reune resultados
ts_cpoint_tbl <-
  dplyr::tibble(index = 1:400,
                ts = ts_sim,
                ts_level = ts_levels)

# Grafico
ggplot_ts_cpoint <- ts_cpoint_tbl %>%
  ggplot(aes(x = index, y = ts_sim)) +
  geom_line() +
  geom_line(y = ts_levels, color = 'red') +
  theme_bw()

plotly::ggplotly(ggplot_ts_cpoint)
```

# Descomposici√≥n de Series de Tiempo

Hemos evidenciado que las series de tiempo pueden exhibir un sinn√∫mero de patrones, 
tales como tendencia, estacionalidad y ciclos, por mencionar algunos. Cuando 
*descomponemos* series de tiempo, lo hacemos mediante la combinaci√≥n de la tendencia
y ciclicidad en un s√≥lo componente tendencia-ciclidad^[En general s√≥lo "tendencia".],
un componente estacional (pudiendo existir m√°s de una componente estacional o ninguna) y un componente residual, que contiene la informaci√≥n restante de la serie de tiempo. 

```{r}
timetk::m4_hourly %>%
  filter(id == "H10") %>%
  timetk::plot_stl_diagnostics(
    date, value,
    .feature_set = c("observed", "season", "trend", "remainder"),
    .frequency   = "24 hours",
    .trend       = "1 week",
    .interactive = TRUE)
```

La extracci√≥n de componentes a partir de una serie de tiempo permite no s√≥lo mejorar
el entendimiento de una serie de tiempo, sino tambi√©n ser utilizado para mejorar la 
elaboraci√≥n de pron√≥sticos

En el ejemplo anterior^[(https://business-science.github.io/timetk/reference/plot_stl_diagnostics.html)] pudimos ver la aplicaci√≥n de la *descomposici√≥n en tendencia y estacionalidad por LOESS ([locally estimated scatterplot smoothing](https://en.wikipedia.org/wiki/Local_regression))*^[üìå Es bueno conocer el trabajo de B.D. Ripley.] (STL), que permite descomponer de manera aditiva^[por defecto, si no se aplican transformaciones tipo Box-Cox] los componentes. La regresi√≥n local
es de utilidad ya que nos permite aplicar un suavizador no-param√©trico realizando
ajustes por m√≠nimos cuadrados en vecindades de una serie num√©rica^[üìå Es posible [optimizar sus hiper-par√°metros (i.e `span`)](http://r-statistics.co/Loess-Regression-With-R.html)]. 

# Medidas de dependencia

## Autocorrelaci√≥n

As√≠ como la correlaci√≥n mide el grado de relaci√≥n lineal entre dos variables,
la *autocorrelaci√≥n*^[a.k.a. Correlaci√≥n Serial] mide la relaci√≥n lineal entre valores rezagados de una serie de tiempo.

<aside>
Autocorrelaci√≥n
</aside>

Existen varios coeficientes de autocorrelaci√≥n, cada uno correspondiente a cada
panel obtenido en un *lag plot*, donde se muestra $y_t$ con respecto a 
$y_{t-k}$ para diferentes valores de $k$ como sigue:

```{r ggplot_airpassengers_lag}
ggplot_airpassengers_lag <- AirPassengers %>%
  tsibble::as_tsibble() %>%
  feasts::gg_lag(value,geom = "point",lags = 1:12) +
  theme_bw()

plotly::ggplotly(ggplot_airpassengers_lag)
```

Aqu√≠ los colores indican el mes de la vairable en el eje vertical, mientras
que los rezagos est√°n graficados en el eje horizontal.

Si $r_k$ mide la relaci√≥n entre $y_t$ y $y_{t-k}$, entonces la autocorrelaci√≥n
del rezago $k$,$r_k$, puede escribirse como:

$$r_{k} = \frac{\sum\limits_{t=k+1}^T (y_{t}-\bar{y})(y_{t-k}-\bar{y})}
 {\sum\limits_{t=1}^T (y_{t}-\bar{y})^2}$$

donde $T$ es el largo de la serie de tiempo. Los coeficientes de autocorrelaci√≥n
consolidan la *funci√≥n de autocorrelaci√≥n* (ACF) de las serie de tiempo.

Revisemos las ACF de las series de tiempo vistas la [clase pasada](https://hapst.netlify.app/posts/clase-01/):

::::: {.panelset .sideways}

::: {.panel}
[Nile]{.panel-name}
```{r acf_nile, echo = FALSE}
# Yearly
Nile %>% 
  tsibble::as_tsibble() %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 20,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[AirPassengers]{.panel-name}
```{r acf_airpassengers}
# Quarterly
AirPassengers %>%
  tsibble::as_tsibble() %>%
  mutate(index = as.Date(index)) %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 12,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Taylor]{.panel-name}
```{r acf_taylor}
# Half-hourly
forecast::taylor %>% 
  tsibble::as_tsibble() %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 48*14,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[EuStockMarkets]{.panel-name}
```{r acf_eu_stock_markets}
# Daily
# DAX: Germany
# SMI: Switzerland
# CAC: France
# FTSE: UK
EuStockMarkets %>%
  timetk::tk_tbl() %>%
  select(DAX) %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = DAX,
    .lags = 30,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[M4-q10]{.panel-name}
```{r acf_q10_quarterly}
# Quarterly
timetk::m4_quarterly %>% filter(id == "Q10") %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 12,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Sunspots]{.panel-name}
```{r acf_sunspots}
# Monthly
datasets::sunspots %>%
  timetk::tk_tbl() %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 12,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Wine]{.panel-name}
```{r acf_wine}
# Monthly
forecast::wineind %>%
  timetk::tk_tbl() %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 24,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Global Temperature]{.panel-name}
```{r acf_gtemp}
# Yearly average global temperature deviations
astsa::gtemp_land %>% 
  timetk::tk_tbl() %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 10,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Speech]{.panel-name}
```{r acf_speech}
# Speech recording of the syllable aaahhh sampled at 10,000 points per second
astsa::speech %>%
  timetk::tk_tbl() %>%
  mutate(index = 1:length(astsa::speech)) %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 200,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Earthquake]{.panel-name}
```{r ggplot_quake}
# 40 points per second
astsa::eqexp$EQ5 %>%
  timetk::tk_tbl() %>%
  mutate(index = 1:length(astsa::eqexp$EQ5)) %>%
  timetk::plot_acf_diagnostics(.date_var = index,
                               .value = data,
                               .lags = 40*10,
                               .interactive = TRUE
  )
```
:::
:::::

Usualmente graficamos la ACF para ver c√≥mo las correlaciones var√≠an con respecto
al lag $k$-√©simo. En la literatura el gr√°fico de la ACF es conocido como *correlograma*.

Al analizar una ACF nos interesar√° enfocarnos en los siguientes fen√≥menos^[M√°s adelante
ahondaremos en los modelos que se podr√≠an utilizar a partir del an√°lisis de una ACF]:

+ $r_k$ es m√°s alto para un $k$ en particular con respecto a los dem√°s. Usualmente
asociado a patrones estacionales en los datos.
+ Autocorrelaciones altas y positivas y lentamente decrecientes o *persistentes* (@Montgomery2015-fr) asociadas a tendencia en la serie de tiempo, usualmente por la cercan√≠a en el tiempo de las observaciones.
+ Dependiendo del paquete de `R`, el correlograma puede incluir intervalos de confianza
dibujados en forma de cono, usualmente asociados al 90% ($\pm 1.96/\sqrt{T}$) o 95%, sugiriendo que valores de correlaci√≥n fuera de este intervalo se asocian con alta probabilidad a correlaci√≥n y no a casualidad estad√≠stica.

## Autocorrelaci√≥n Parcial

Las autocorrelaciones entre rezagos est√°n formadas por los efectos de correlaci√≥n
directos e indirectos. La *autocorrelaci√≥n parcial* (PACF) para el rezago $k$ es la correlaci√≥n que resulta luego de remover el efecto de cualquier correlaci√≥n asociada a los t√©rminos de rezagos intermedios (@Cowpertwait2009-qv), permitiendo
aislar los efectos directos de cada rezago $k$ en los valores actuales de la serie analizada.

# Modelos de Series de Tiempo üöÄ

El grado de suavidad de las series de tiempo revisadas en la [clase pasada](https://hapst.netlify.app/posts/clase-01/index.html#introST) es una de las caracter√≠sticas
fundamentales que nos permite diferenciarlas entre ellas. Como vimos anteriormente,
podr√≠amos suponer que los puntos adyacentes en el tiempo est√°n *correlacionados*,
por lo que el valor de la serie en el per√≠odo $t$, $y_t$, depender√≠a de alg√∫n modo
de sus valores pasados $y_{t-1}, y_{t-2},\dots$. Podemos incorporar dicha suposici√≥n
junto a colecciones de variables aleatorias para modelar series de tiempo.

A continuaci√≥n revisaremos la serie m√°s simple que podemos generar: una colecci√≥n
de variables aleatorias no correlacionadas utilizando la distribuci√≥n normal.

## Ruido Blanco

El *ruido blanco*^[En ingl√©s White Noise (WN), en analog√≠a con la luz blanca y la idea
de que todas las oscilaciones peri√≥dicas posibles est√°n presentes en ella con igual fuerza.]
es una de las series de tiempo m√°s simples que podemos generar y es ampliamente
utilizada como modelo de ruido en aplicaciones ingenieriles. Corresponde a 
observaciones aleatorias, independientes e identicamente distribuidas, lo que los
estad√≠sticos llaman variables aleatorias *iid* (*independent and identically distributed*).

<aside>
Ruido Blanco (iid)
</aside>

Un ruido blanco particularmente √∫til es el *ruido blanco Gaussiano*, donde
$w_t$ son variables aleatorias independientes con media 0 y varianza $\sigma_{w}^{2}$,
lo que podemos escribir como $w_t \sim \text{iid}\ N(0,\sigma_{w}^{2})$.

Anteriormente utilizamos `rnorm()` para generar posibles realizaciones de una serie de
tiempo. Dicha serie no pose√≠a tendencia, estacionalidad ni ciclicidades aparentes, por
lo que vale la pena preguntarse *¬øc√≥mo ser√° su ACF?*üí° 

```{r}
ts_tbl %>% 
  filter(series == "ts_1") %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 100,
    .interactive = TRUE
  )

```

Podemos ver que la serie no posee ning√∫n tipo de correlaci√≥n, s√≥lo ruido. No existe
informaci√≥n para poder construir un modelo de pron√≥stico. Para evaluar nuestra hip√≥tesis,
podemos utilizar el test de Ljung-Box (`Box.test()`)^[[https://koalatea.io/r-ljung-box-test/](https://koalatea.io/r-ljung-box-test/)], que considera la autocorrelaci√≥n de los primeros $h$ valores juntos. La significancia del test indica que los datos probablemente no son
ruido blanco.

## Estacionariedad

El ruido blanco es el ejemplo m√°s simple de un *proceso estacionario*. Una serie de tiempo
se dice *estrictamente estacionaria* si sus propiedades no se ven afectadas un cambio
en el origen del tiempo. Es decir, la distribuci√≥n de probabilidad conjunta de las observaciones
$y_t, y_{t+1}, \dots, y_{t+n}$ es exactamente la misma que la distribuci√≥n de probabilidad
conjunta de $y_t+k, y_{t+k+1}, \dots, y_{t+k+n}$. 

<aside>
Estacionariedad
</aside>

La estacionariedad implica un tipo de *equilibrio* o *estabilidad* estad√≠stica en los datos. Por ello, la serie de tiempo posee una media constante definida de manera usual

$$\mu_y = E(y) = \int_{-\infty}^{\infty}yf(y)dy$$
y varianza constante definida como

$$\sigma_{y}^{2} = \text{Var}(y) = \int_{-\infty}^{\infty}(y - \mu_{y}^{})^{2}f(y)dy$$

La media y varianza muestral pueden ser utilizadas para estimar dichos par√°metros. Si 
las observaciones en una serie de tiempo son $y_1, y_2, \dots, y_{T}$, entonces la media
muestral es 

$$\bar{y} = \hat{\mu}_{y} = \frac{1}{T}\sum_{t=1}^{T}y_t$$
y la varianza muestral^[No hay mucha diferencia entre usar $T$ y $T-1$ cuando se tienen grandes candidades de observaciones] es 

$$s^{2} = \hat{\sigma}_{y}^{2} = \frac{1}{T}\sum_{t=1}^{T}(y_t - \bar{y})^{2}$$

### Estacionariedad d√©bil

Para que la ACF tenga sentido, la serie debe ser considerada una serie *debilmente estacionaria*. Esto implica que la funci√≥n de autocorrelaci√≥n para cualquier rezago
particular es la misma sin importar el lugar en el que estamos en el tiempo. La serie
ser√° debilmente estacionaria si 

+ La media $E(y_t)$ es la misma para todo $t$
+ La varianza de $y_t$ es la misma para todo $t$.
+ La covarianza (y la correlaci√≥n) entre $y_t$ y $y_{t-k}$ es la misma
para todo $t$ en cada rezago $k = 1,2,3 \dots$

En t√©rminos generales, se habla de que una serie es estacionaria si no existen cambios sistem√°ticos en la media (no posee tendencia), en su varianza y si las variaciones estrictamente peri√≥dicas pueden ser removidas. Es usual que las series estudiadas violen la estacionariedad como propiedad. Sin embargo, esta denominaci√≥n se utiliza a menudo para expresar que la serie posee caracter√≠sticas que sugieren el ajuste de un modelo estacionario. Gran parte de la teor√≠a probabil√≠stica asociada a series de tiempo se asocia al an√°lisis de series de tiempo estacionarias, por lo que dicho an√°lisis a implicar√° transformar una serie no-estacionaria en una estacionaria mediante la remoci√≥n de tendencia y variaci√≥n estacional de los datos, para luego modelar la variaci√≥n en los residuales mediante un proceso estoc√°stico estacionario.

# Transformaciones

La visualizaci√≥n de una serie de tiempo puede sugerir la aplicaci√≥n de una transformaci√≥n, por ejemplo, aplicando logaritmo o ra√≠z cuadrada. Las principales razones por las cuales transformar una serie de tiempo son (@Chatfield2019-jt): 

1. Estabilizar la varianza
  
  Si hay tendencia en la serie y la varianza parece incrementarse con la media puede ser recomendable transformar los datos. En particular, si la desviaci√≥n est√°ndar es directamente proporcional a la media, una transformaci√≥n logar√≠tmica peude ser adecuada. Por otro lado, si la varianza cambia a trav√©s del tiempo *sin* una tendencia presente en a serie, la transformaci√≥n no ser√° de utilidad y quiz√° valga la pena evaluar modelos que admitan cambios en la varianza^[[Modelos Heterosced√°sticos y aproximaciones a cl√∫sters de volatilidad](https://stats.stackexchange.com/questions/169244/testing-a-single-time-series-for-changing-variance-structure-heteroscedasticity)].
  
2. Obtenci√≥n de efectos estacionales aditivos^[Retomaremos esta idea en la clase 4.]

  Si existe tendencia en la serie y el tama√±o del efecto estacional parece incrementarse con la media, puede ser recomendable transformar los datos para lograr que el efecto estacional sea constante de a√±o a a√±o. En este √∫ltimo caso, se dice que el efecto estacional es aditivo. En el caso de que el efecto estacional sea directamente proporcional a la media, entonces el efecto estacional es considerado multiplicativo y un transformaci√≥n logar√≠tmica es apropiada para hacerlo aditivo^[Sin embargo, la transformaci√≥n solo estabilizar√° la varianza si el t√©rmino del error tambi√©n es pensado como multiplicativo, lo que a veces se pasa por alto.].

3. Lograr una distribuci√≥n normal de los datos

  La construcci√≥n de modelos y pron√≥sticos usualmente se basa en el supuesto de que los datos distribuyen normal. En la pr√°ctica es usual que ese no sea el caso, por ejemplo, puede haber evidencia de sesgos asociados a *picos* en la misma direcci√≥n en la serie de tiempo (hacia arriba o hacia abajo). Este efecto puede ser dif√≠cil de eliminar con una transformaci√≥n y quiz√° sea necesario modelar los datos con una distribuci√≥n del error distinta.
  
## Transformaci√≥n de Box-Cox

Las transformaciones logar√≠tmicas y de ra√≠ces cuadradas mencionadas anteriormente son casos especiales de una clase general de transformaciones llamadas las *Transformaciones de Box-Cox*. Dada un serie de tiempo observada ${y_t}$ y un par√°metro de transformaci√≥n $\lambda$, la serie transformadad est√° dada por 

<aside>
Transformaciones de Box-Cox
</aside>

\begin{equation}
  y_t  =
    \begin{cases}
      \log(x_t) & \text{si $\lambda=0$}  \\
      (x_t^{\lambda} - 1)/\lambda & \text{si $\lambda \neq 0$}
    \end{cases}
\end{equation}

En el caso en que $\lambda \neq 0$, se tiene una transformaci√≥n de potencias en la que $y_t$ es una funci√≥n continua de $\lambda$ en el caso $\lambda=0$^[El valor de $\lambda$ usualmente es obtenido mediante m√°xima verosimilitud]. En cualquier caso, es necesario tener en mente que una transformaci√≥n de este tipo hace m√°s dif√≠cil interpretar y pronosticar valores producidos por el modelo transformado y quiz√° sea necesario "retornar" los valores a su escala original para su uso, lo que puede inducir sesgos.

---

```{r}
sessionInfo()
```
