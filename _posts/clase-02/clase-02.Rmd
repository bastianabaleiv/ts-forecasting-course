---
title: "Clase 2"
description: |
  Series de Tiempo: Conceptos, An치lisis, Manipulaci칩n y Visualizaci칩n
author:
  - name: Basti치n Aballay L.
    url: https://www.linkedin.com/in/bastianaballay/
date: "2022-01-17"
bibliography: clase-02.bib
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: true
    highlight: tango
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r packages, include=FALSE}
suppressPackageStartupMessages({
  library(tidyverse)
  library(plotly)
  library(xaringanExtra)
})

xaringanExtra::use_panelset()
```

En la [Clase 1](https://hapst.netlify.app/posts/clase-01/#introST) introdujimos las series de tiempo de
manera conceptual como una colecci칩n de observaciones secuenciales en el tiempo.

En esta clase revisaremos las series de tiempo desde una perspectiva te칩rica,
estableciendo los antecendentes estad칤sticos necesarios para analizarlas y
caracterizarlas. Luego de esta clase podr치s:

+ Entender las series de tiempo como *realizaciones* de procesos estoc치sticos
+ Caracterizar series de tiempo utilizando medidas de dependencia y descomposiciones
+ Proponer visualizaciones adecuadas a la serie de tiempo estudiada

Por supuesto, todo lo anterior a trav칠s de la utilizaci칩n de distintos paquetes 
de `R`.

# Series de Tiempo 游늳

Uno de los objetivos principales del an치lisis de series de tiempo es desarrollar
modelos matem치ticos que provean de descripciones plausibles para los datos
muestreados. Para establecer una configuraci칩n estad칤stica que nos permita describir
las series de tiempo, asumiremos que una serie de tiempo puede ser definida como
una colecci칩n de variables aleatorias indexadas de acuerdo al orden en que fueron
obtenidas a trav칠s del tiempo.

<aside>
Series de Tiempo
</aside>

Podemos entonces entender una serie de tiempo como la secuencia de variables aleatorias $y_1,
y_2,y_3,\dots$, donde la variable aleatoria $y_1$ denota el valor que toma la
serie en la primera observaci칩n temporal, la variable $y_2$ denota el valor que
toma el segundo punto en el tiempo y as칤 sucesivamente. Podemos referirnos a dicha secuencia de 
variables aleatorias $\{y_t\} = \{y_1,\dots,y_T\}$, indexada por $t$^[El 칤ndice $t$ usualmente es discreto y toma valores en los enteros] como un *proceso estoc치stico*^[[Cross 
Validated: Is a time series the same as a stochastic process?](https://stats.stackexchange.com/questions/126791/is-a-time-series-the-same-as-a-stochastic-process)]. Luego, los valores observados de una serie de tiempo son una *realizaci칩n* del proceso estoc치stico, siendo una de muchas posibles secuencias que un proceso aleatorio puede generar (@Shumway2017-dp).

<aside>
Proceso estoc치stico
</aside>

Otra manera de ver a las series de tiempo es como una muestra finita que se obtiene de 
una secuencia doblemente infinita subyacente: $\{\dots, y_{-1},y_0,y_1,y_2,\dots,y_T,y_{T+1},y_{T+2},\dots\}$.

En este contexto, un *modelo de series de tiempo* para $\{y_t\}$  es una especificaci칩n
de las distribuciones conjuntas de la sencuencia de variables aleatorias para la cual
$\{y_t\}$ es una realizaci칩n. Dado lo anterior, un modelo de series de tiempo nos permitir치
usar una realizaci칩n para realizar inferencias acerca de la distribuci칩n conjunta
subyacente desde donde se produjo la realizaci칩n obtenida.


```{r}
set.seed(42)

ts_tbl <-  replicate(10, rnorm(300, mean = 0, sd = 1)) %>%
  as_tibble() %>%
  rename_all( ~ paste0("ts_", 1:10)) %>%
  mutate(index = 1:300) %>% 
  pivot_longer(
    cols = -c(index),
    names_to = "series"
  ) %>% 
  mutate(realization = ifelse(series == "ts_1","Yes","No"))
```

::::: {.panelset}

::: {.panel}
[Distribuci칩n de Realizaciones]{.panel-name}
```{r ggplot_sim_ts_all, echo = FALSE}
ggplot_sim_ts_all <- ts_tbl %>% 
  ggplot(
    aes(x = index, y = value, color = series)
  ) +
  geom_line() +
  theme_bw()  

plotly::ggplotly(ggplot_sim_ts_all)

```
:::

::: {.panel}
[Realizaci칩n en particular]{.panel-name}
```{r ggplot_sim_ts_real}
ggplot_sim_ts_real <- ts_tbl %>% 
  ggplot(
    aes(x = index, y = value, color = realization)
  ) +
  geom_line() +
  scale_color_manual(values=c("lightgray","black")) +
  theme_bw()  
  
  plotly::ggplotly(ggplot_sim_ts_real)
```
:::
:::::

# Componentes de una Serie de Tiempo

En la [Clase 1](https://hapst.netlify.app/posts/clase-01/) describimos al an치lisis exploratorio como uno de los objetivos
principales del an치lisis de series de tiempo, haciendo 칠nfasis en la necesidad de 
identificar dichos fen칩menos para poder caracterizar las series estudiadas. 
A continuaci칩n definimos de manera m치s formal a qu칠 nos referiremos cuando 
hablemos de tendencia, estacionalidad y ciclos, entre otros t칠rminos (@Hyndman2021-hc).

## Patrones regulares

### Tendencia

Hablamos de que existe *tendencia* en una serie de tiempo cuando hay un patr칩n
creciente o decreciente a largo plazo^[쯈u칠 es largo plazo de todas maneras?
Para el clima una variaci칩n c칤clica puede ocurrir en un per칤odo de 50 a침os. Si
s칩lo tuvieramos 20 a침os de datos, esta oscilaci칩n podr칤a parecer una tendencia...]
en los datos. Cuando la tendencia cambia de una tendencia creciente a decreciente 
hablamos de un *cambio en la direcci칩n* de la serie. Al analizar la tendencia, es 
necesario considerar la cantidad de observaciones disponible y realizar una
evaluaci칩n subjetiva de a qu칠 nos referimos con *largo plazo*. Existen
m칠todos para estimar o remover la tendencia y as칤 observar de mejor manera
las otras fuentes de variaci칩n de una serie. Finalmente, si la serie no posee tendencia podemos considerarla *estacionaria*^[Retomaremos este concepto de manera m치s formal un poco m치s
adelante] en la media.

<aside>
<p>Tendencia</p>
<p>Estacionariedad</p>
</aside>

```{r}
# Global mean land-ocean temperature deviations (from 1951-1980 average), measured in degrees centigrade, for the years 1880-2015.
ggplot_globtemp <-
  astsa::globtemp %>% 
  timetk::tk_tbl() %>% 
  ggplot(aes(x = index, y = value)) +
  geom_line()

plotly::ggplotly(ggplot_globtemp)
```

### Estacionalidad

Un patr칩n estacional ocurre cuando una serie de tiempo es afectada por factores
estacionales tales como el per칤odo del a침o o el d칤a de la semana. Es decir,
hablamos de una serie con *estacionalidad*  cuando la serie muestra variaciones
sistem치ticas a trav칠s de un per칤odo de tiempo determinado. La estacionalidad
en general es fija y de per칤odo conocido. Este tipo de variaci칩n es f치cil de 
entender y puede ser facilmente estimado si el efecto estacional es de inter칠s directo.
De manera alternativa, tambi칠n es posible remover la variaci칩n estacional de los
datos, para obtener datos *desestacionalizados*, si dicha variaci칩n no es de inter칠s.

<aside>
Estacionalidad
</aside>

```{r}
ggplot_airpassengers <- AirPassengers %>%
  tsibble::as_tsibble() %>%
  mutate(index = as.Date(index)) %>% 
  ggplot((aes(x = index, y = value))) +
  geom_line() +
  theme_bw()

plotly::ggplotly(ggplot_airpassengers)
```


### Ciclicidad

Un *ciclo* ocurre cuando los datos exhiben alzas y bajas que no poseen frecuencia
determinada. Estas fluctuaciones continuas en la tendencia se asocian usualmente
a condiciones econ칩micas determinadas por el tipo de industria o negocio estudiado.
La duraci칩n usual de estas fluctuaciones es de al menos 2 a침os.

> Si las fluctuaciones presentes en una serie de tiempo no poseen una frecuencia
determinada son consideradas c칤clicas. Si la frecuencia es invariante y asociada
a alg칰n aspecto del calendario, entonces el patr칩n es estacional.

## Patrones irregulares

### Irregularidades

Luego de que las variaciones asociadas a patrones de tendencia o estacionalidad
han sido removidos de un set de datos, obtenemos una serie de *residuales*^[Retomaremos
este concepto de manera m치s formal un poco m치s adelante]
que pueden parecer (o no) aleatorios. Los elementos irregulares de una serie de 
tiempo le dan sus caracter칤sticas no-sistem치ticas. Al hacer pron칩sticos, la
idea es *calibrar* cada uno de los componentes de una serie de tiempo en una 
manera precisas excepto por el componente irregular.

<aside>
Residual
</aside>

### Outliers

Desde una perspectiva tradicional, una anomal칤a u *outlier* es una observaci칩n
que se desv칤a con respecto a las otras observaciones lo suficiente como para
generar sospechas acerca de su proceso de generaci칩n. Es decir, un outlier
es una observaciones que no sigue un comportamiento esperado. Si la observaci칩n
es indeseada (por ejemplo un error de medici칩n producto de un sensor
descalibrado o evento de ocurrencia 칰nica en el calendario), usualmente podemos 
limpiarla o imputarla. Sin embargo, si el evento es de inter칠s, quiz치 sea necesario
analizar el outlier de manera aislada (por ejemplo en detecci칩n de fraude).

<aside>
Outliers
</aside>

```{r ggplot_btc_usd_anoms, fig.show = 'hide'}
# devtools::install_github("amrrs/coindeskr")
library(coindeskr)

# Obtenemos precio historico del Bitcoin en USd췂
btc_usd_tbl <-
  get_historic_price('USD', '2019-01-01', '2021-10-25') %>%
  timetk::tk_tbl() %>%
  rename(Date = index)

# devtools::install_github("twitter/AnomalyDetection")
# Deteccion de anomalias
library(AnomalyDetection)
btc_usd_anoms <-
  AnomalyDetectionTs(
    btc_usd_tbl,
    max_anoms = 0.05,
    direction = 'both',
    plot = FALSE
  )

# Graficamos
ggplot_btc_usd_anoms <- btc_usd_anoms$anoms %>%
  as_tibble() %>%
  mutate(Date = as.Date(timestamp), .keep = "unused") %>%
  left_join(btc_usd_tbl, .) %>%
  ggplot(aes(x = Date, y = Price)) +
  geom_line() +
  geom_point(aes(y = anoms), color = 'red') +
  theme_bw()

plotly::ggplotly(ggplot_btc_usd_anoms)
```


### Cambios estructurales

Un cambio estructural (a veces llamados cambios de r칠gimen) es un cambio repentino e inesperado en el comportamiento de una serie de tiempo. En t칠rminos estad칤sticos, un cambio estructural ocurre cuando la distribuci칩n de probabilidad subyacente de una serie de tiempo cambia. El proceso de detecci칩n de puntos de cambios^[V칠ase paquete [`changepoint`](https://www.lancs.ac.uk/~killick/Pub/KillickEckley2011.pdf)] busca identificar cuando ocurren estos cambios, usualmente utilizando
algoritmos que comparan propiedades estad칤sticas de la distribuci칩n nueva con respecto a la original.

```{r}
library(changepoint)

set.seed(42)

# Simulamos a partir de una distribucion normal
ts_sim <-
  c(rnorm(100, mean = 0, sd = 1),
    rnorm(100, mean = 1, sd = 1),
    rnorm(100, mean = 0, sd = 1),
    rnorm(100, mean = 0.2, sd = 1))

# Calculamos posicionamiento optimo y cantidad (potencial) de
# puntos de cambio en los datos usando PELT
ts_pelt <- cpt.mean(ts_sim, method='PELT')

# [!] Notar que ts_pelt es un objeto clase S4
# Establecemos la media para cada intervalo hallado
ts_levels <-
  rep(ts_pelt@param.est$mean, 
      times = c(ts_pelt@cpts[1], diff(ts_pelt@cpts)))

# Generamos tibble que reune resultados
ts_cpoint_tbl <-
  dplyr::tibble(index = 1:400,
                ts = ts_sim,
                ts_level = ts_levels)

# Grafico
ggplot_ts_cpoint <- ts_cpoint_tbl %>%
  ggplot(aes(x = index, y = ts_sim)) +
  geom_line() +
  geom_line(y = ts_levels, color = 'red') +
  theme_bw()

plotly::ggplotly(ggplot_ts_cpoint)
```

# Descomposici칩n de Series de Tiempo

Hemos evidenciado que las series de tiempo pueden exhibir un sinn칰mero de patrones, 
tales como tendencia, estacionalidad y ciclos, por mencionar algunos. Cuando 
*descomponemos* series de tiempo, lo hacemos mediante la combinaci칩n de la tendencia
y ciclicidad en un s칩lo componente tendencia-ciclidad^[En general s칩lo "tendencia".],
un componente estacional (pudiendo existir m치s de una componente estacional o ninguna) y un componente residual, que contiene la informaci칩n restante de la serie de tiempo. 

```{r}
timetk::m4_hourly %>%
  filter(id == "H10") %>%
  timetk::plot_stl_diagnostics(
    date, value,
    .feature_set = c("observed", "season", "trend", "remainder"),
    .frequency   = "24 hours",
    .trend       = "1 week",
    .interactive = TRUE)
```

La extracci칩n de componentes a partir de una serie de tiempo permite no s칩lo mejorar
el entendimiento de una serie de tiempo, sino tambi칠n ser utilizado para mejorar la 
elaboraci칩n de pron칩sticos

En el ejemplo anterior^[(https://business-science.github.io/timetk/reference/plot_stl_diagnostics.html)] pudimos ver la aplicaci칩n de la *descomposici칩n en tendencia y estacionalidad por LOESS ([locally estimated scatterplot smoothing](https://en.wikipedia.org/wiki/Local_regression))*^[游늷 Es bueno conocer el trabajo de B.D. Ripley.] (STL), que permite descomponer de manera aditiva^[por defecto, si no se aplican transformaciones tipo Box-Cox] los componentes. La regresi칩n local
es de utilidad ya que nos permite aplicar un suavizador no-param칠trico realizando
ajustes por m칤nimos cuadrados en vecindades de una serie num칠rica^[游늷 Es posible [optimizar sus hiper-par치metros (i.e `span`)](http://r-statistics.co/Loess-Regression-With-R.html)]. 

# Medidas de dependencia

## Autocorrelaci칩n

As칤 como la correlaci칩n mide el grado de relaci칩n lineal entre dos variables,
la *autocorrelaci칩n*^[a.k.a. Correlaci칩n Serial] mide la relaci칩n lineal entre valores rezagados de una serie de tiempo.

<aside>
Autocorrelaci칩n
</aside>

Existen varios coeficientes de autocorrelaci칩n, cada uno correspondiente a cada
panel obtenido en un *lag plot*, donde se muestra $y_t$ con respecto a 
$y_{t-k}$ para diferentes valores de $k$ como sigue:

```{r ggplot_airpassengers_lag}
ggplot_airpassengers_lag <- AirPassengers %>%
  tsibble::as_tsibble() %>%
  feasts::gg_lag(value,geom = "point",lags = 1:12) +
  theme_bw()

plotly::ggplotly(ggplot_airpassengers_lag)
```

Aqu칤 los colores indican el mes de la vairable en el eje vertical, mientras
que los rezagos est치n graficados en el eje horizontal.

Si $r_k$ mide la relaci칩n entre $y_t$ y $y_{t-k}$, entonces la autocorrelaci칩n
del rezago $k$,$r_k$, puede escribirse como:

$$r_{k} = \frac{\sum\limits_{t=k+1}^T (y_{t}-\bar{y})(y_{t-k}-\bar{y})}
 {\sum\limits_{t=1}^T (y_{t}-\bar{y})^2}$$

donde $T$ es el largo de la serie de tiempo. Los coeficientes de autocorrelaci칩n
consolidan la *funci칩n de autocorrelaci칩n* (ACF) de las serie de tiempo.

Revisemos las ACF de las series de tiempo vistas la [clase pasada](https://hapst.netlify.app/posts/clase-01/):

::::: {.panelset .sideways}

::: {.panel}
[Nile]{.panel-name}
```{r acf_nile, echo = FALSE}
# Yearly
Nile %>% 
  tsibble::as_tsibble() %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 20,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[AirPassengers]{.panel-name}
```{r acf_airpassengers}
# Quarterly
AirPassengers %>%
  tsibble::as_tsibble() %>%
  mutate(index = as.Date(index)) %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 12,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Taylor]{.panel-name}
```{r acf_taylor}
# Half-hourly
forecast::taylor %>% 
  tsibble::as_tsibble() %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 48*14,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[EuStockMarkets]{.panel-name}
```{r acf_eu_stock_markets}
# Daily
# DAX: Germany
# SMI: Switzerland
# CAC: France
# FTSE: UK
EuStockMarkets %>%
  timetk::tk_tbl() %>%
  select(DAX) %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = DAX,
    .lags = 30,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[M4-q10]{.panel-name}
```{r acf_q10_quarterly}
# Quarterly
timetk::m4_quarterly %>% filter(id == "Q10") %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 12,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Sunspots]{.panel-name}
```{r acf_sunspots}
# Monthly
datasets::sunspots %>%
  timetk::tk_tbl() %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 12,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Wine]{.panel-name}
```{r acf_wine}
# Monthly
forecast::wineind %>%
  timetk::tk_tbl() %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 24,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Global Temperature]{.panel-name}
```{r acf_gtemp}
# Yearly average global temperature deviations
astsa::gtemp_land %>% 
  timetk::tk_tbl() %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 10,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Speech]{.panel-name}
```{r acf_speech}
# Speech recording of the syllable aaahhh sampled at 10,000 points per second
astsa::speech %>%
  timetk::tk_tbl() %>%
  mutate(index = 1:length(astsa::speech)) %>%
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 200,
    .interactive = TRUE
  )
```
:::

::: {.panel}
[Earthquake]{.panel-name}
```{r ggplot_quake}
# 40 points per second
astsa::eqexp$EQ5 %>%
  timetk::tk_tbl() %>%
  mutate(index = 1:length(astsa::eqexp$EQ5)) %>%
  timetk::plot_acf_diagnostics(.date_var = index,
                               .value = data,
                               .lags = 40*10,
                               .interactive = TRUE
  )
```
:::
:::::

Usualmente graficamos la ACF para ver c칩mo las correlaciones var칤an con respecto
al lag $k$-칠simo. En la literatura el gr치fico de la ACF es conocido como *correlograma*.

Al analizar una ACF nos interesar치 enfocarnos en los siguientes fen칩menos^[M치s adelante
ahondaremos en los modelos que se podr칤an utilizar a partir del an치lisis de una ACF]:

+ $r_k$ es m치s alto para un $k$ en particular con respecto a los dem치s. Usualmente
asociado a patrones estacionales en los datos.
+ Autocorrelaciones altas y positivas y lentamente decrecientes o *persistentes* (@Montgomery2015-fr) asociadas a tendencia en la serie de tiempo, usualmente por la cercan칤a en el tiempo de las observaciones.
+ Dependiendo del paquete de `R`, el correlograma puede incluir intervalos de confianza
dibujados en forma de cono, usualmente asociados al 90% ($\pm 1.96/\sqrt{T}$) o 95%, sugiriendo que valores de correlaci칩n fuera de este intervalo se asocian con alta probabilidad a correlaci칩n y no a casualidad estad칤stica.

## Autocorrelaci칩n Parcial

Las autocorrelaciones entre rezagos est치n formadas por los efectos de correlaci칩n
directos e indirectos. La *autocorrelaci칩n parcial* (PACF) para el rezago $k$ es la correlaci칩n que resulta luego de remover el efecto de cualquier correlaci칩n asociada a los t칠rminos de rezagos intermedios (@Cowpertwait2009-qv), permitiendo
aislar los efectos directos de cada rezago $k$ en los valores actuales de la serie analizada.

# Modelos de Series de Tiempo 游

El grado de suavidad de las series de tiempo revisadas en la [clase pasada](https://hapst.netlify.app/posts/clase-01/index.html#introST) es una de las caracter칤sticas
fundamentales que nos permite diferenciarlas entre ellas. Como vimos anteriormente,
podr칤amos suponer que los puntos adyacentes en el tiempo est치n *correlacionados*,
por lo que el valor de la serie en el per칤odo $t$, $y_t$, depender칤a de alg칰n modo
de sus valores pasados $y_{t-1}, y_{t-2},\dots$. Podemos incorporar dicha suposici칩n
junto a colecciones de variables aleatorias para modelar series de tiempo.

A continuaci칩n revisaremos la serie m치s simple que podemos generar: una colecci칩n
de variables aleatorias no correlacionadas utilizando la distribuci칩n normal.

## Ruido Blanco

El *ruido blanco*^[En ingl칠s White Noise (WN), en analog칤a con la luz blanca y la idea
de que todas las oscilaciones peri칩dicas posibles est치n presentes en ella con igual fuerza.]
es una de las series de tiempo m치s simples que podemos generar y es ampliamente
utilizada como modelo de ruido en aplicaciones ingenieriles. Corresponde a 
observaciones aleatorias, independientes e identicamente distribuidas, lo que los
estad칤sticos llaman variables aleatorias *iid* (*independent and identically distributed*).

<aside>
Ruido Blanco (iid)
</aside>

Un ruido blanco particularmente 칰til es el *ruido blanco Gaussiano*, donde
$w_t$ son variables aleatorias independientes con media 0 y varianza $\sigma_{w}^{2}$,
lo que podemos escribir como $w_t \sim \text{iid}\ N(0,\sigma_{w}^{2})$.

Anteriormente utilizamos `rnorm()` para generar posibles realizaciones de una serie de
tiempo. Dicha serie no pose칤a tendencia, estacionalidad ni ciclicidades aparentes, por
lo que vale la pena preguntarse *쯖칩mo ser치 su ACF?*游눠 

```{r}
ts_tbl %>% 
  filter(series == "ts_1") %>% 
  timetk::plot_acf_diagnostics(
    .date_var = index,
    .value = value,
    .lags = 100,
    .interactive = TRUE
  )

```

Podemos ver que la serie no posee ning칰n tipo de correlaci칩n, s칩lo ruido. No existe
informaci칩n para poder construir un modelo de pron칩stico. Para evaluar nuestra hip칩tesis,
podemos utilizar el test de Ljung-Box (`Box.test()`)^[[https://koalatea.io/r-ljung-box-test/](https://koalatea.io/r-ljung-box-test/)], que considera la autocorrelaci칩n de los primeros $h$ valores juntos. La significancia del test indica que los datos probablemente no son
ruido blanco.

## Estacionariedad

El ruido blanco es el ejemplo m치s simple de un *proceso estacionario*. Una serie de tiempo
se dice *estrictamente estacionaria* si sus propiedades no se ven afectadas un cambio
en el origen del tiempo. Es decir, la distribuci칩n de probabilidad conjunta de las observaciones
$y_t, y_{t+1}, \dots, y_{t+n}$ es exactamente la misma que la distribuci칩n de probabilidad
conjunta de $y_t+k, y_{t+k+1}, \dots, y_{t+k+n}$. 

<aside>
Estacionariedad
</aside>

La estacionariedad implica un tipo de *equilibrio* o *estabilidad* estad칤stica en los datos. Por ello, la serie de tiempo posee una media constante definida de manera usual

$$\mu_y = E(y) = \int_{-\infty}^{\infty}yf(y)dy$$
y varianza constante definida como

$$\sigma_{y}^{2} = \text{Var}(y) = \int_{-\infty}^{\infty}(y - \mu_{y}^{})^{2}f(y)dy$$

La media y varianza muestral pueden ser utilizadas para estimar dichos par치metros. Si 
las observaciones en una serie de tiempo son $y_1, y_2, \dots, y_{T}$, entonces la media
muestral es 

$$\bar{y} = \hat{\mu}_{y} = \frac{1}{T}\sum_{t=1}^{T}y_t$$
y la varianza muestral^[No hay mucha diferencia entre usar $T$ y $T-1$ cuando se tienen grandes candidades de observaciones] es 

$$s^{2} = \hat{\sigma}_{y}^{2} = \frac{1}{T}\sum_{t=1}^{T}(y_t - \bar{y})^{2}$$

### Estacionariedad d칠bil

Para que la ACF tenga sentido, la serie debe ser considerada una serie *debilmente estacionaria*. Esto implica que la funci칩n de autocorrelaci칩n para cualquier rezago
particular es la misma sin importar el lugar en el que estamos en el tiempo. La serie
ser치 debilmente estacionaria si 

+ La media $E(y_t)$ es la misma para todo $t$
+ La varianza de $y_t$ es la misma para todo $t$.
+ La covarianza (y la correlaci칩n) entre $y_t$ y $y_{t-k}$ es la misma
para todo $t$ en cada rezago $k = 1,2,3 \dots$

En t칠rminos generales, se habla de que una serie es estacionaria si no existen cambios sistem치ticos en la media (no posee tendencia), en su varianza y si las variaciones estrictamente peri칩dicas pueden ser removidas. Es usual que las series estudiadas violen la estacionariedad como propiedad. Sin embargo, esta denominaci칩n se utiliza a menudo para expresar que la serie posee caracter칤sticas que sugieren el ajuste de un modelo estacionario. Gran parte de la teor칤a probabil칤stica asociada a series de tiempo se asocia al an치lisis de series de tiempo estacionarias, por lo que dicho an치lisis a implicar치 transformar una serie no-estacionaria en una estacionaria mediante la remoci칩n de tendencia y variaci칩n estacional de los datos, para luego modelar la variaci칩n en los residuales mediante un proceso estoc치stico estacionario.

# Transformaciones

La visualizaci칩n de una serie de tiempo puede sugerir la aplicaci칩n de una transformaci칩n, por ejemplo, aplicando logaritmo o ra칤z cuadrada. Las principales razones por las cuales transformar una serie de tiempo son (@Chatfield2019-jt): 

1. Estabilizar la varianza
  
  Si hay tendencia en la serie y la varianza parece incrementarse con la media puede ser recomendable transformar los datos. En particular, si la desviaci칩n est치ndar es directamente proporcional a la media, una transformaci칩n logar칤tmica peude ser adecuada. Por otro lado, si la varianza cambia a trav칠s del tiempo *sin* una tendencia presente en a serie, la transformaci칩n no ser치 de utilidad y quiz치 valga la pena evaluar modelos que admitan cambios en la varianza^[[Modelos Heterosced치sticos y aproximaciones a cl칰sters de volatilidad](https://stats.stackexchange.com/questions/169244/testing-a-single-time-series-for-changing-variance-structure-heteroscedasticity)].
  
2. Obtenci칩n de efectos estacionales aditivos^[Retomaremos esta idea en la clase 4.]

  Si existe tendencia en la serie y el tama침o del efecto estacional parece incrementarse con la media, puede ser recomendable transformar los datos para lograr que el efecto estacional sea constante de a침o a a침o. En este 칰ltimo caso, se dice que el efecto estacional es aditivo. En el caso de que el efecto estacional sea directamente proporcional a la media, entonces el efecto estacional es considerado multiplicativo y un transformaci칩n logar칤tmica es apropiada para hacerlo aditivo^[Sin embargo, la transformaci칩n solo estabilizar치 la varianza si el t칠rmino del error tambi칠n es pensado como multiplicativo, lo que a veces se pasa por alto.].

3. Lograr una distribuci칩n normal de los datos

  La construcci칩n de modelos y pron칩sticos usualmente se basa en el supuesto de que los datos distribuyen normal. En la pr치ctica es usual que ese no sea el caso, por ejemplo, puede haber evidencia de sesgos asociados a *picos* en la misma direcci칩n en la serie de tiempo (hacia arriba o hacia abajo). Este efecto puede ser dif칤cil de eliminar con una transformaci칩n y quiz치 sea necesario modelar los datos con una distribuci칩n del error distinta.
  
## Transformaci칩n de Box-Cox

Las transformaciones logar칤tmicas y de ra칤ces cuadradas mencionadas anteriormente son casos especiales de una clase general de transformaciones llamadas las *Transformaciones de Box-Cox*. Dada un serie de tiempo observada ${y_t}$ y un par치metro de transformaci칩n $\lambda$, la serie transformadad est치 dada por 

<aside>
Transformaciones de Box-Cox
</aside>

\begin{equation}
  y_t  =
    \begin{cases}
      \log(x_t) & \text{si $\lambda=0$}  \\
      (x_t^{\lambda} - 1)/\lambda & \text{si $\lambda \neq 0$}
    \end{cases}
\end{equation}

En el caso en que $\lambda \neq 0$, se tiene una transformaci칩n de potencias en la que $y_t$ es una funci칩n continua de $\lambda$ en el caso $\lambda=0$^[El valor de $\lambda$ usualmente es obtenido mediante m치xima verosimilitud]. En cualquier caso, es necesario tener en mente que una transformaci칩n de este tipo hace m치s dif칤cil interpretar y pronosticar valores producidos por el modelo transformado y quiz치 sea necesario "retornar" los valores a su escala original para su uso, lo que puede inducir sesgos.

---

```{r}
sessionInfo()
```
